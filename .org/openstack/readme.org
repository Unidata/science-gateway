#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: auto-id:t

#+TITLE: readme
#+DATE: <2017-03-02 Thu>
#+AUTHOR: Julien Chastang
#+EMAIL: chastang@ucar.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.2 (Org mode 9.0.5)

* Running VMs on Jetstream with OpenStack
  :PROPERTIES:
  :CUSTOM_ID: h-90A8A74D
  :END:
** Introduction
   :PROPERTIES:
   :CUSTOM_ID: h-11F59F95
   :END:

It is preferable to interface with the NSF Jetstream cloud via the [[https://use.jetstream-cloud.org/application/dashboard][Atmosphere web interface]]. However, this web dashboard is limited in two important ways:

  1. Users cannot obtain VMs with static IPs
  2. Users cannot open low number ports (i.e., < ~1024~) with the exception of ports ~22~, ~80~ and ~443~ which are open by default.

If you are in either of these scenarios, you have to interface with Jetstream via the [[https://iujetstream.atlassian.net/wiki/display/JWT/Using+the+Jetstream+API][OpenStack API]]. The problem here is that there is some initial setup which is somewhat painful but that you do only once. We provide a Docker container that hopefully eases some of the pain as it has the OpenStack command line tools and convenience scripts installed. After that setup, you can interface with the Jetstream OpenStack command line via the =openstack.sh= script which will launch a Docker container that will enable you to:

  - Create IP Numbers
  - Create VMs
  - Tear down VMs
  - Create Data Volumes
  - Attach Data Volumes
  - Open TCP ports

Note, you do not have to employ this Docker container. It is merely provided as a convenience. If you choose, you can go through the Jetstream OpenStack API instructions directly and ignore all that follows.

** Install Docker (Do This Once)
   :PROPERTIES:
   :CUSTOM_ID: h-DE5B47F1
   :END:

Install Docker via the ~rocky-init.sh~ script because we will be interacting with the OpenStack Jetstream API via Docker. This step should make our lives easier.

** Clone the science-gateway Repository (Do This Once)
   :PROPERTIES:
   :CUSTOM_ID: h-968FA51C
   :END:

We will be making heavy use of the ~Unidata/science-gateway~ git repository.

#+BEGIN_SRC sh :eval no
  git clone https://github.com/Unidata/science-gateway
#+END_SRC

** Pull or Build Docker Container (Do This Once)
   :PROPERTIES:
   :CUSTOM_ID: h-4A9632CC
   :END:

#+BEGIN_SRC sh :eval no
  cd science-gateway/openstack
#+END_SRC

At this point, you can either pull or build the ~science-gateway~ container:

*** Pull Container
    :PROPERTIES:
    :CUSTOM_ID: h-B5690030
    :END:
#+BEGIN_SRC sh :eval no
  docker pull unidata/science-gateway
#+END_SRC

*** Build Container
    :PROPERTIES:
    :CUSTOM_ID: h-1C54F677
    :END:
#+BEGIN_SRC sh :eval no
  docker build -t unidata/science-gateway .
#+END_SRC

** API Setup
   :PROPERTIES:
   :CUSTOM_ID: h-CBD5EC54
   :END:

We will be using the Jetstream API directly and via convenience scripts.

*** Download and Edit openrc.sh (Do This Once)
    :PROPERTIES:
    :CUSTOM_ID: h-8B3E8EEE
    :END:

The next part involves downloading the =openrc.sh= file to work with our OpenStack allocation. You will have first login to the OpenStack TACC dashboard which will necessitate a password reset. Unfortunately, this login is not the same as the Jetstream Atmosphere web interface login. Also, follow the usual password advice of not reusing passwords as this password will end up in your OpenStack environment and [[#h-9C0700C5][you may want to add it]] in the =openrc.sh= file for convenience.

**** [[https://portal.tacc.utexas.edu/password-reset/][Reset your OpenStack TACC dashboard password]]
     :PROPERTIES:
     :CUSTOM_ID: h-3E2185E5
     :END:

**** Download your =openrc.sh= file from the IU (not TACC) dashboard at  [[https://iu.jetstream-cloud.org]] and move it to the =openstack/bin= directory.
     :PROPERTIES:
     :CUSTOM_ID: h-B34CC3AF
     :END:

   - See /"Use the Horizon dashboard to generate openrc.sh"/ in the [[https://iujetstream.atlassian.net/wiki/display/JWT/Setting+up+openrc.sh][Jetstream API instructions]].
   - From the [[https://iu.jetstream-cloud.org/project/api_access/][IU dashboard]], navigate to =Project=, =API Access=, then select =Download OpenStack RC File= at top-right.
   - Select *OpenStack RC File (Identity API 3)* , which will download as a script named something like =TG-ATM160027-openrc.sh=. You should rename it to =openrc.sh=.
   - Move this file to =bin/openrc.sh= (e.g., =/home/jane/science-gateway/openstack/bin/openrc.sh=).

**** Edit =bin/openrc.sh= Password (Optional)
     :PROPERTIES:
     :CUSTOM_ID: h-9C0700C5
     :END:

For convenience, you may wish to add your password to the =openrc.sh= file. Again, follow the usual advice of not reusing passwords as this password will end up in your OpenStack environment.

Edit the =openrc.sh= file and the supply the TACC resource =OS_PASSWORD= you [[#h-8B3E8EEE][reset earlier]]:

#+BEGIN_SRC sh :eval no
  export OS_PASSWORD="changeme!"
#+END_SRC

Comment out

#+BEGIN_SRC sh :eval no
# echo "Please enter your OpenStack Password: "
# read -sr OS_PASSWORD_INPUT
#+END_SRC

*** Fire Up Container and More Setup
    :PROPERTIES:
    :CUSTOM_ID: h-30B73273
    :END:
**** openstack.sh
     :PROPERTIES:
     :CUSTOM_ID: h-5F4AFF6F
     :END:

Start the ~unidata/science-gateway~ container with =openstack.sh= convenience script. The script take a ~-o~ argument for your =openrc.sh= file and a ~-s~ argument for the directory containing or will contain your ssh keys (e.g., =/home/jane/science-gateway/openstack/ssh= or a new directory that will contain contain your Jetstream OpenStack keys that we will be creating shortly). *Both arguments must be supplied with fully qualified path names.*

#+BEGIN_SRC sh :eval no
  chmod +x openstack.sh
  ./openstack.sh -o </path/to/your openrc.sh file> -s </path/to/your/ssh directory>
#+END_SRC

Subsequently, when interacting with Jetstream via OpenStack API now and in the future, you will be using this container to create VMs, mount volumes, etc.

A wrapper script =run.sh= is provided, which assumes that directories =bin/= and =ssh/= exist in the working directory, and that =bin/= contains =openrc.sh=:

#+BEGIN_SRC sh
  ./run.sh
#+END_SRC

You can use this =run.sh= script as a template for you to parameterize, perhaps for alternative =openrc.sh= files.

**** Create ssh Keys (Do This Once)
     :PROPERTIES:
     :CUSTOM_ID: h-EE48476C
     :END:

This step of ssh key generation is important. In our experience, we have not had good luck with preexisting keys. You may have to generate a new one. Be careful with the ~-f~ argument below. We are operating under one allocation so make sure your key names do not collide with other users. Name your key something like ~<some short somewhat unique id>-${OS_PROJECT_NAME}-api-key~. Then you add your public key the TACC dashboard with ~openstack keypair create~.

 #+BEGIN_SRC sh :eval no
  cd ~/.ssh
  ssh-keygen -b 2048 -t rsa -f <key-name> -P ""
  openstack keypair create --public-key <key-name>.pub <key-name>
  # go back to home directory
  cd
 #+END_SRC

The =ssh= directory was mounted from outside the Docker container you are currently running. Your public/private key should be saved there. Don't lose it or else you may not be able to delete the VMs you are about to create.

**** Testing Setup
     :PROPERTIES:
     :CUSTOM_ID: h-257FBBBE
     :END:

At this point, you should be able to run ~openstack image list~ which should yield something like:

#+TBLNAME: image-list
| ID                                   | Name                               |
|--------------------------------------+------------------------------------|
| fd4bf587-39e6-4640-b459-96471c9edb5c | AutoDock Vina Launch at Boot       |
| 02217ab0-3ee0-444e-b16e-8fbdae4ed33f | AutoDock Vina with ChemBridge Data |
| b40b2ef5-23e9-4305-8372-35e891e55fc5 | BioLinux 8                         |
|--------------------------------------+------------------------------------|

If not, check your setup.

** Working with Jetstream API to Create VMs
   :PROPERTIES:
   :CUSTOM_ID: h-03303143
   :END:

At this point, we are past the hard work. You will employ the ~unidata/science-gateway~ container accessed via the =openstack.sh= convenience script to

  - Create IP Numbers
  - Create VMs
  - Tear down VMs
  - Create Data Volumes
  - Attach Data Volumes

If you have not done so already:

#+BEGIN_SRC sh :eval no
  ./openstack.sh -o </path/to/your openrc.sh file> -s </path/to/your/ssh directory>
#+END_SRC

*** IP Numbers
    :PROPERTIES:
    :CUSTOM_ID: h-5E7A7E65
    :END:

We are ready to fire up VMs. First create an IP number which we will be using shortly:

#+BEGIN_SRC sh :eval no
  openstack floating ip create public
  openstack floating ip list
#+END_SRC

or you can just ~openstack floating ip list~ if you have IP numbers left around from previous VMs.

*** Boot VM
    :PROPERTIES:
    :CUSTOM_ID: h-EA17C2D9
    :END:

**** Create VM
    :PROPERTIES:
    :CUSTOM_ID: h-7E8034E7
    :END:
Now you can boot up a VM with something like the following command:

#+BEGIN_SRC sh :eval no
  boot.sh -n unicloud -k <key-name> -s m1.medium -ip 149.165.157.137
#+END_SRC

The ~boot.sh~ command takes a VM name, [[#h-EE48476C][ssh key name]] defined earlier, size, and IP number created earlier, and optionally an image UID which can be obtained with ~openstack image list | grep -i featured~. Note that these feature VMs are recommended by Jetstream staff, and have a default user corresponding to the Linux distribution flavor. For example,

#+BEGIN_SRC sh :eval no
$ openstack image list | grep -i featured
#+END_SRC

may yield something like:

#+BEGIN_SRC sh :eval no
| 45405d78-e108-48bf-a502-14a0dab81915 | Featured-RockyLinux8 | active |
| e85293e8-c9b0-4fc9-88b6-e3645c7d1ad3 | Featured-Ubuntu18    | active |
| 49d5e275-23d6-44b5-aa60-94242d92caf1 | Featured-Ubuntu20    | active |
| e41dc578-b911-48c6-a468-e607a8b2c87c | Featured-Ubuntu22    | active |
#+END_SRC

The Rocky VMs will have a default of user ~rocky~ and the Ubuntu VMs will have a default user of ~ubuntu~.

Also see ~boot.sh -h~ and ~openstack flavor list~ for more information.

**** SSH Into New VM
    :PROPERTIES:
    :CUSTOM_ID: h-10ACA1BC
    :END:

At this point, you can ~ssh~ into our newly minted VM. Explicitly providing the key name with the ~ssh~ ~-i~ argument and a user name (e.g., ~rocky~) may be important:

#+BEGIN_SRC sh :eval no
  ssh -i ~/.ssh/<key-name> rocky@149.165.157.137
#+END_SRC

At this point, you might see

#+BEGIN_SRC sh :eval no
  ssh: connect to host 149.165.157.137 port 22: No route to host
#+END_SRC

Usually waiting for a few minutes resolves the issue. If you are still have trouble, try ~openstack server stop <vm-uid-number>~ followed by ~openstack server start <vm-uid-number>~.

**** Adding Additional SSH Keys (Optional)
     :PROPERTIES:
     :CUSTOM_ID: h-A66BED33
     :END:

Once you are in your VM, it is probably best to add additional ssh public keys into the ~authorized_keys~ file to make logging in easier from whatever host you are connecting from.

*** Create and Attach Data Volumes
    :PROPERTIES:
    :CUSTOM_ID: h-9BEEAB97
    :END:

You can create data volumes via the OpenStack API. As an example, here, we will be creating a 750GB ~data~ volume. You will subsequently attach the data volume:

#+BEGIN_SRC sh :eval no
  openstack volume create --size 750 data

  openstack volume list && openstack server list

  openstack server add volume <vm-uid-number> <volume-uid-number>
#+END_SRC

You will then be able to log in to your VM and mount your data volume with typical Unix ~mount~, ~umount~, and ~df~ commands. If running these command manually (not using the =mount.sh= script) you will need to run ~kfs.ext4 /dev/sdb~ to create an ~ext4~ partition using the entire disk.

There is a ~mount.sh~ convenience script to mount *uninitialized* data volumes. Run this script as root or ~sudo~ on the newly created VM not from the OpenStack CL.

**** Ensure Volume Availability Upon Machine Restart
     :PROPERTIES:
     :CUSTOM_ID: h-F6AF5F18
     :END:

You want to ensure data volumes are available when the VM starts (for example after a reboot). To achieve this objective, you can run this command which will add an entry to the ~/etc/fstab~ file:

#+BEGIN_SRC shell :eval no
  echo UUID=2c571c6b-c190-49bb-b13f-392e984a4f7e /data ext4 defaults 1 1 | tee \
      --append /etc/fstab > /dev/null
#+END_SRC

where the ~UUID~ represents the ID of the data volume device name (e.g., ~/dev/sdb~) which you can discover with the ~blkid~ (or ~ls -la /dev/disk/by-uuid~) command. [[https://askubuntu.com/questions/164926/how-to-make-partitions-mount-at-startup-in-ubuntu-12-04][askubuntu]] has a good discussion on this topic.

*** Opening TCP Ports
    :PROPERTIES:
    :CUSTOM_ID: h-D6B1D4C2
    :END:

Opening TCP ports on VMs must be done via OpenStack with the ~openstack security group~ command line interfaces. In addition, this can be For example, to create a security group that will enable the opening of TCP port ~80~:

#+BEGIN_SRC sh :eval no
  secgroup.sh -n my-vm-ports -p 80
#+END_SRC

Once the security group is created, you can attach multiple TCP ports to that security group with ~openstack security group~ commands. For example, here we are attaching port ~8080~ to the ~global-my-vm-ports~ security group.

#+BEGIN_SRC sh :eval no
  openstack security group rule create global-my-vm-ports --protocol tcp --dst-port 8080:8080 --remote-ip 0.0.0.0/0
#+END_SRC

Finally, you can attach the security group to the VM (e.g., ~my-vm~) with:

#+BEGIN_SRC sh :eval no
  openstack server add security group my-vm global-my-vm-ports
#+END_SRC
*** Dynamic DNS and Recordsets
:PROPERTIES:
:CUSTOM_ID: h-612458CB
:END:

JetStream2 handles dynamic DNS differently than JetStream1; domain names will look like ~<instance-name>.<project-ID>.projects.jetstream-cloud.org~. In addition, domain names are assigned automatically when a floating IP is assigned to a VM which is on a network with the ~dns-domain~ property set.

To set this property when manually creating a network, run the following openstack command. Note the (necessary) trailing "." at the end of the domain:

~openstack network create <new-network-name> --dns-domain <project-ID>.projects.jetstream-cloud.org.~

To set this property on an existing network:

~openstack network set --dns-domain <project-ID>.projects.jetstream-cloud.org. <network-name>~

When creating a new VM using [[./bin/boot.sh][boot.sh]], the VM is added to the ~unidata-public~ network, which should already have the ~dns_domain~ property set. To confirm this for any network, run a:

~openstack network show <network>~

If you wanted to manually create/edit domain names, do so using the ~openstack recordset~ commands. Note that you must have ~python-designateclient~ [[https://docs.openstack.org/python-designateclient/latest/user/shell-v2.html][installed]].

#+BEGIN_SRC shell
  # See the current state of your project's DNS zone
  # Useful for getting IDs of individual recordsets
  openstack recordset list <project-ID>.projects.jetstream-cloud.org.

  # More closely inspect a given recordset
  openstack recordset show <project-ID>.projects.jetstream-cloud.org. <recordset-ID>

  # Create new DNS record
  openstack recordset create \
    --record <floating-ip-of-instance> \
    --type A \
    <project-ID>.projects.jetstream-cloud.org. \
    <your-desired-hostname>.<project-ID>.projects.jetstream-cloud.org.

  # Remove an unused record (because you created a new one for it, or otherwise)
  openstack recordset delete <project-ID>.projects.jetstream-cloud.org. <old-recordset-ID>
#+END_SRC
*** Tearing Down VMs
    :PROPERTIES:
    :CUSTOM_ID: h-1B38941F
    :END:
**** umount External Volumes
     :PROPERTIES:
     :CUSTOM_ID: h-B367439E
     :END:

There is also a ~teardown.sh~ convenience script for deleting VMs. Be sure to ~umount~ any data volumes before deleting a VM. For example on the VM in question,

#+BEGIN_SRC sh :eval no
  umount /data
#+END_SRC

You may have to verify, here, that nothing is writing to that data volume such as Docker or NFS (e.g., ~docker-compose stop~, ~sudo service nfs-kernel-server stop~), in case you get errors about the volume being busy.

In addition, just to be on the safe side, remove the volume from the VM via OpenStack:

#+BEGIN_SRC sh :eval no
  openstack volume list && openstack server list

  openstack server remove volume <vm-uid-number> <volume-uid-number>
#+END_SRC

**** Tear Down
     :PROPERTIES:
     :CUSTOM_ID: h-8FDA03F6
     :END:

Then finally from the OpenStack CL,

#+BEGIN_SRC sh :eval no
  teardown.sh -n unicloud -ip 149.165.157.137
#+END_SRC

For now, you have to supply the IP number even though the script should theoretically be smart enough to figure that out.
*** Swapping VMs
    :PROPERTIES:
    :CUSTOM_ID: h-56B1F4AC
    :END:

Cloud-computing promotes the notion of the throwaway VM. We can swap in VMs that will have the same IP address and attached volume disk storage. However, before swapping out VMs, we should do a bit of homework and careful preparation so that the swap can go as smoothly as possible.

**** Prerequisites
     :PROPERTIES:
     :CUSTOM_ID: h-82627F76
     :END:

Create the VM that will be swapped in. Make sure to:
 - initialize the new VM with the ~rocky-init.sh~ script
 - build or fetch relevant Docker containers
 - copy over the relevant configuration files. E.g., check with ~git diff~ and scrutinize ~~/config~
 - check the crontab with ~crontab -l~
 - beware of any ~10.0~ address changes that need to be made (e.g., NFS mounts)
 - consider other ancillary stuff (e.g., check home directory, ~docker-compose~ files)
 - think before you type

**** /etc/fstab and umount
     :PROPERTIES:
     :CUSTOM_ID: h-5122BD67
     :END:

Examine =/etc/fstab= to find all relevant mounts on "old" VM. Copy over =fstab= to new host (the ~UUIDs~ should remain the same but double check). Then ~umount~ mounts.

**** OpenStack Swap
     :PROPERTIES:
     :CUSTOM_ID: h-45D6670A
     :END:

From the OpenStack command line, identify the VM IDs of the old and new VM as well as any attached external volume ID:

#+BEGIN_SRC shell :eval no
  openstack volume list && openstack server list
#+END_SRC

#+BEGIN_SRC shell :exports none :shebang "#!/bin/bash" :tangle "../../openstack/bin/swap-vm.sh"

  echo Make sure to:
  echo  - initialize new VM
  echo  - open the same ports
  echo  - build or fetch relevant Docker containers
  echo  - copy over the relevant configuration files. E.g., check with git diff and scrutinize ~/config
  echo  - check the crontab with crontab -l
  echo  - beware of any 10.0 address changes that need to be made \(e.g., NFS mounts\)
  echo  - consider other ancillary stuff \(e.g., check home directory, docker-compose files\)
  echo  - think before you type

  read -p "Are you sure you want to continue? " -n 1 -r
  echo
  if [[ ! $REPLY =~ ^[Yy]$ ]]
  then
      [[ "$0" = "$BASH_SOURCE" ]] && exit 1 || return 1
  fi

  usage="$(basename "$0") [-h] [-o, --old old VM ID] [-n, --new new VM ID] \n
      [-v, --volume zero or more volume IDs (each supplied with -v)] \n
      [-ip, --ip ip address] \n
      -- script to swap VMs:\n
      -h  show this help text\n
      -o, --old old VM ID\n
      -n, --new new VM ID\n
      -v, --volume zero or more volume IDs (each supplied with -v)\n
      -ip, --ip VM ip number\n"

  while [[ $# > 0 ]]
  do
      key="$1"
      case $key in
          -o|--old)
              VM_ID_OLD="$2"
              shift # past argument
              ;;
          -n|--new)
              VM_ID_NEW="$2"
              shift # past argument
              ;;
          -v|--volumes)
              VOLUME_IDS+="$2 "
              shift # past argument
              ;;
          -ip|--ip)
              IP="$2"
              shift # past argument
              ;;
          -h|--help)
              echo -e $usage
              exit
              ;;
      esac
      shift # past argument or value
  done

  if [ -z "$VM_ID_OLD" ];
    then
        echo "Must supply a vm name:"
        echo -e $usage
        exit 1
  fi

  if [ -z "$VM_ID_NEW" ];
    then
        echo "Must supply a key name:"
        echo -e $usage
        exit 1
  fi

  if [ -z "$IP" ];
     then
        echo "Must supply an IP address:"
        echo -e $usage
        echo openstack floating ip list
        exit 1
  fi
#+END_SRC

Then swap out both the IP address as well as zero or more external data volumes with the new server.

#+BEGIN_SRC shell :tangle "../../openstack/bin/swap-vm.sh"

  openstack server remove floating ip ${VM_ID_OLD} ${IP}
  openstack server add floating ip ${VM_ID_NEW} ${IP}

  for i in ${VOLUME_IDS}
  do
       openstack server remove volume ${VM_ID_OLD} $i
       openstack server add volume ${VM_ID_NEW} $i
  done
#+END_SRC

**** /etc/fstab and mount
     :PROPERTIES:
     :CUSTOM_ID: h-152E6DAB
     :END:

Issue ~blkid~ (or ~ls -la /dev/disk/by-uuid~) command to find ~UUIDs~ that will be inserted into the =/etc/fstab=. Lastly, ~mount -a~.
** Building a Kubernetes Cluster
   :PROPERTIES:
   :CUSTOM_ID: h-DA34BC11
   :END:

It is possible to create a Kubernetes cluster with the Docker container described here. We employ [[https://github.com/zonca/jetstream_kubespray][Andrea Zonca's modification of the kubespray project]]. Andrea's recipe to build a Kubernetes cluster on Jetstream with kubespray is described [[https://zonca.dev/2022/03/kubernetes-jetstream2-kubespray.html][here]].  These instructions have been codified with the ~kube-setup.sh~ and ~kube-setup2.sh~ scripts.

Make sure to run both ~kubectl~ and ~helm~ from the client and ~ssh~ tunnel (~ssh ubuntu@FLOATINGIPOFMASTER -L 6443:localhost:6443~) into the master node as described in the instructions.

*** Define cluster with cluster.tfvars
    :PROPERTIES:
    :CUSTOM_ID: h-F44D1317
    :END:

First, set the ~CLUSTER~ name environment variable (named "k8s-unidata", for example) for the current shell and all processes started from the current shell. It will be referenced by various scripts. This step is done for you by supplying the ~--name~ argument to ~jupyterhub.sh~ and subsequently ~z2j.sh~ (see  [[here][../vms/jupyter/readme.md]]). However, if you want to do this manually, run this from within the docker container launched by ~jupyterhub.sh~:

#+BEGIN_SRC sh
  export CLUSTER="$CLUSTER"
#+END_SRC

Then, modify =~/jetstream_kubespray/inventory/kubejetstream/cluster.tfvars= to specify the number of nodes in the cluster and the size ([[#h-958EA909][flavor]]) of the VMs. For example,

#+BEGIN_SRC sh
  # nodes
  number_of_k8s_nodes = 0
  number_of_k8s_nodes_no_floating_ip = 2
  flavor_k8s_node = "4"
#+END_SRC

will create a 2 node cluster of ~m1.large~ VMs. [[https://zonca.github.io/2022/03/kubernetes-jetstream-kubespray.html][See Andrea's instructions for more details]].

[[https://docs.google.com/spreadsheets/d/15qngBz4L5gwv_JX9HlHsD4iT25Odam09qG3JzNNbdl8/edit?usp=sharing][This spreadsheet]] will help you determine the size of the cluster based on number of users, desired cpu/user, desired RAM/user. Duplicate it and adjust it for your purposes.

~openstack flavor list~ will give the IDs of the desired VM size.

Also, note that ~cluster.tfvars~ assumes you are building a cluster at the TACC data center with the sections pertaining to IU commented out. If you would like to set up a cluster at IU, make the necessary modifications located at the end of ~cluster.tfvars~.

*IMPORTANT*: once you define an ~image~ (e.g., ~image = JS-API-Featured-Ubuntu18-May-22-2019~) or a flavor size (e.g., ~flavor_k8s_master = 2~), make sure you do not subsequently change it after you have run Terraform and Ansible!  This scenario can happen when [[#h-1991828D][adding cluster nodes]] and the featured image no longer exists because it has been updated. If you must change these values, you'll first have to [[file:../vms/jupyter/readme.org::#h-5F2AA05F][preserve your application data]] and do a [[#h-DABDACC7][gentle - IP preserving - cluster tear down]] before rebuilding it and re-installing your application.

*** Enable Dynamic DNS with cluster.tfvars
:PROPERTIES:
:CUSTOM_ID: h-7801DD3F
:END:

JetStream2 handles dynamic DNS differently than JetStream1; domain names will look like ~<instance-name>.<project-ID>.projects.jetstream-cloud.org~. In addition, domain names are assigned automatically when a floating IP is assigned to a VM which is on a network with the ~dns-domain~ property set.

To configure terraform to set this property, add/edit the line below in ~cluster.tfvars~.

#+BEGIN_SRC shell
  # Uncomment below and edit to set dns-domain network property
  # network_dns_domain = "<project-ID>.projects.jetstream-cloud.org."
#+END_SRC

Note the (necessary) trailing "." at the end of the domain.

After running the terraform scripts (see the next section), you can ensure that the dns name was correctly assigned to your cluster's master node with:

#+BEGIN_SRC shell
  nslookup <instance-name>.<project-ID>.projects.jetstream-cloud.org
#+END_SRC

*** Create VMs with kube-setup.sh
    :PROPERTIES:
    :CUSTOM_ID: h-0C658E7B
    :END:

At this point, to create the VMs that will house the kubernetes cluster  run

~kube-setup.sh~

This script essentially wraps Terraform install scripts to launch the VMs according to ~cluster.tfvars~.

Once, the script is complete,  let the VMs settle for a while (let's say ten minutes). Behind the scenes ~dpkg~ is running on the newly created VMs which can take some time to complete.

**** Check Status of VMs
     :PROPERTIES:
     :CUSTOM_ID: h-136A4851
     :END:

Check to see the status of the VMs with:

#+BEGIN_SRC sh
  openstack server list | grep $CLUSTER
#+END_SRC

and

#+BEGIN_SRC sh
  watch -n 15 \
       ansible -i $HOME/jetstream_kubespray/inventory/$CLUSTER/hosts -m ping all
#+END_SRC

***** Ansible Timeouts
:PROPERTIES:
:CUSTOM_ID: h-2B239C73
:END:
The ansible script works via ~sudo~. That escalation can lead to timeout errors if ~sudo~ is not fast enough. For example:

#+begin_src shell
  fatal: [gpu-test3-1]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
  fatal: [gpu-test3-k8s-node-nf-1]: FAILED! => {"msg": "Timeout (12s) waiting for privilege escalation prompt: "}
#+end_src

In that case add

#+begin_src shell
  timeout = 60
  gather_timeout = 60
#+end_src

under the ~[default]~ tag in =jetstream_kubespray/ansible.cfg=.

***** Steps if VMs are Unhappy
     :PROPERTIES:
     :CUSTOM_ID: h-F4401658
     :END:

If the check status process did not go smoothly, here are some thing you can try to remedy the problem.

If you see any errors, you can try to wait a bit more or reboot the offending VM with:

#+BEGIN_SRC sh
  openstack server reboot <vm>
#+END_SRC

or you can reboot all VMs with:

#+BEGIN_SRC sh
  osl | grep $CLUSTER | awk '{print $2}' | xargs -n1 openstack server reboot
#+END_SRC

If VMs stuck in ~ERROR~ state. You may be able to fix this problem with:

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray/inventory/$CLUSTER/
  sh terraform_apply.sh
#+END_SRC

or you can destroy the VMs and try again

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray/inventory/$CLUSTER/
  sh terraform_destroy.sh
#+END_SRC

*** Install Kubernetes with kube-setup2.sh
    :PROPERTIES:
    :CUSTOM_ID: h-05F9D0A2
    :END:

Next, run

#+BEGIN_SRC sh
  kube-setup2.sh
#+END_SRC

If seeing errors related to ~dpkg~, wait and try again or [[#h-F4401658][try these steps]].

Run ~kube-setup2.sh~ again.

*** Check Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-D833684A
    :END:

Ensure the Kubernetes cluster is running:

#+BEGIN_SRC
  kubectl get pods --all-namespaces
#+END_SRC

and get a list of the nodes:

#+BEGIN_SRC sh
  kubectl get nodes --all-namespaces
#+END_SRC

*** Adding Nodes to Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-1991828D
    :END:

! *THINK before you type here because if you scale with an updated Ubuntu VM ID with respect to what is running on the cluster, you may wipe out your cluster* ! [[https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/issues/54][See the GitHub issue about this topic]].

You can augment the computational capacity of your cluster by adding nodes. In theory, this is just a simple matter of [[#h-F44D1317][adding worker nodes]] in =jetstream_kubespray/inventory/$CLUSTER/cluster.tfvars= followed by running:

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray/inventory/$CLUSTER/
  sh terraform_apply.sh
#+END_SRC

Wait a bit to allow ~dpkg~ to finish running on the new node(s). [[#h-136A4851][Check the VMS]]. Next:

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray
  sleep 1000; sh k8s_scale.sh
#+END_SRC

[[#h-D833684A][Check the cluster]].

*** Removing Nodes from Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-0324031E
    :END:

It is also possible to remove nodes from a Kubernetes cluster. First see what nodes are running:

#+BEGIN_SRC sh
  kubectl get nodes --all-namespaces
#+END_SRC

which will yield something like:

#+BEGIN_SRC sh
  NAME                     STATUS   ROLES    AGE   VERSION
  k8s-unidata-k8s-master-1    Ready    master   42h   v1.12.5
  k8s-unidata-k8s-node-nf-1   Ready    node     42h   v1.12.5
  k8s-unidata-k8s-node-nf-2   Ready    node     41h   v1.12.5
#+END_SRC

From the Kubernetes client:

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray
  sh k8s_remove_node.sh k8s-unidata-k8s-node-nf-2
#+END_SRC

followed by running:

#+BEGIN_SRC sh
  teardown.sh -n  k8s-unidata-k8s-node-nf-2
#+END_SRC

from the openstack command line.

If tearing down many nodes/VMs, you can try something like:

#+BEGIN_SRC sh
  for i in {3..10}; do sh k8s_remove_node.sh k8s-unidata-k8s-node-nf-$i; done

  for i in {3..10}; do teardown.sh -n k8s-unidata-k8s-node-nf-$i; done
#+END_SRC

[[#h-D833684A][Check the cluster]].

*** Tearing Down the Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-DABDACC7
    :END:
**** Without Preserving IP of Master Node
     :PROPERTIES:
     :CUSTOM_ID: h-25092B48
     :END:

Once you are finished with your Kubernetes cluster you can completely wipe it out (think before you type and make sure you have the cluster name correct):

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray/inventory/$CLUSTER/
  sh terraform_destroy.sh
#+END_SRC

**** With Preserving IP of Master Node
     :PROPERTIES:
     :CUSTOM_ID: h-AA4B8849
     :END:

You can also tear down your cluster but still preserve the IP number of the master node. This is useful and important when the IP of the master node is associated with a DNS name that you wish to keep associated.

#+BEGIN_SRC sh
  cd ~/jetstream_kubespray/inventory/$CLUSTER/
  sh terraform_destroy_keep_floatingip.sh
#+END_SRC

Subsequently, when you invoke ~terraform_apply.sh~, the master node should have the same IP number as before.

*Note*: AFTER invoking ~terraform_apply.sh~ remove the =~/.ssh/known_hosts= line that corresponds to the old master node! This can easily be achieved by sshing into the new master node which will indicate the offending line in =~/.ssh/known_hosts=. This will avoid headaches when invoking ~kube-setup2.sh~.

*** Monitoring the Cluster with Grafana and Prometheus
    :PROPERTIES:
    :CUSTOM_ID: h-005364BF
    :END:

[[https://grafana.com/][Grafana]] is a monitoring engine equipped with nice dashboards and fancy time-series visualizations. [[https://github.com/camilb/prometheus-kubernetes][Prometheus]] allows for monitoring of Kubernetes clusters.

Installing these monitoring technologies is fairly straightforward and [[https://zonca.github.io/2019/04/kubernetes-monitoring-prometheus-grafana.html][described here]].
*** Patching Master Node
:PROPERTIES:
:CUSTOM_ID: h-9BC6B08B
:END:

    :PROPERTIES:
    :CUSTOM_ID: h-1B2FF6A7
    :END:
You'll want to keep the master node security patched as it will have a publicly accessible IP number attached to a well known DNS name. If you see packages out of date upon login, as root user:

 #+BEGIN_SRC sh :eval no
   apt-get update && apt-get -y upgrade && apt-get -y dist-upgrade \
       && apt autoremove -y
  reboot -h now
 #+END_SRC

* Appendix
  :PROPERTIES:
  :CUSTOM_ID: h-78283D4A
  :END:
** Jetstream2 VM Flavors
   :PROPERTIES:
   :CUSTOM_ID: h-958EA909
   :END:

|----+-----------+---------+------+-------+-----------|
| ID | Name      |     RAM | Disk | VCPUs | Is Public |
|----+-----------+---------+------+-------+-----------|
|  1 | m3.tiny   |    3072 |   20 |     1 | True      |
| 10 | g3.small  |   15360 |   60 |     4 | False     |
| 11 | g3.medium |   30720 |   60 |     8 | False     |
| 12 | g3.large  |   61440 |   60 |    16 | False     |
| 13 | g3.xl     |  128000 |   60 |    32 | False     |
| 14 | r3.large  |  512000 |   60 |    64 | False     |
| 15 | r3.xl     | 1024000 |   60 |   128 | False     |
|  2 | m3.small  |    6144 |   20 |     2 | True      |
|  3 | m3.quad   |   15360 |   20 |     4 | True      |
|  4 | m3.medium |   30720 |   60 |     8 | True      |
|  5 | m3.large  |   61440 |   60 |    16 | True      |
|  7 | m3.xl     |  128000 |   60 |    32 | True      |
|  8 | m3.2xl    |  256000 |   60 |    64 | True      |
|----+-----------+---------+------+-------+-----------|
