#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: auto-id:t
#+TITLE: Creating  a JupyterHub on Jetstream with the Zero to JupyterHub Project
#+DATE:  <2017-06-26 Mon>
#+AUTHOR: Julien Chastang
#+EMAIL: chastang@ucar.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.2 (Org mode 9.0.5)

* Creating  a JupyterHub on Jetstream with the Zero to JupyterHub Project
  :PROPERTIES:
  :CUSTOM_ID: h-D73CBC56
  :END:

** Kubernetes Cluster
   :PROPERTIES:
   :CUSTOM_ID: h-65F9358E
   :END:

*** jupyterhub.sh
    :PROPERTIES:
    :CUSTOM_ID: h-B56E19AB
    :END:

~jupyterhub.sh~ and the related ~z2j.sh~ are convenience scripts similar to ~openstack.sh~ to give you access to a pre-configured environment that will allow you to build and/or run a Zero to JupyterHub cluster. It also relies on the [[file:../../openstack/readme.org::#h-4A9632CC][same Docker container]] as the ~openstack.sh~ script. ~jupyterhub.sh~ takes one argument with the ~-n~ option, the name of the Zero to JupyterHub cluster.  Invoke it from the =science-gateway/openstack= directory. ~jupyterhub.sh~ and the related ~z2j.sh~ ensure the information for this Zero to JupyterHub cluster is persisted outside the container via Docker file mounts -- otherwise all the information about this cluster  would be confined in memory inside the Docker container. The vital information will be persisted in a local =jhub= directory.

*** Create Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-2FF65549
    :END:

[[file:../../openstack/readme.org::#h-DA34BC11][Create a Kubernetes cluster]] with the desired number of nodes and VM sizes. Lock down the master node of the cluster per Unidata security procedures. Work with sys admin staff to obtain a DNS name (e.g., jupyterhub.unidata.ucar.edu), and a certificate from a certificate authority for the master node.

** unidata/unidatahub Docker Container
   :PROPERTIES:
   :CUSTOM_ID: h-CD007D2A
   :END:

Build the Docker container in this directory and push it to dockerhub.

#+BEGIN_SRC sh
  docker build -t unidata/unidatahub:`openssl rand -hex 6` . > /tmp/docker.out 2>&1 &
  docker push unidata/unidatahub:<container id>
#+END_SRC

** Configure and Deploy the JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-E5CA5D99
   :END:

From the client host where you created the Kubernetes cluster, follow [[https://zonca.dev/2020/06/kubernetes-jetstream-kubespray.html#install-jupyterhub][Andrea Zonca's instructions]].

After you have created the ~secrets.yaml~ as instructed, customize it with the choices below

*** Letsencrypt versus Certificate from a Certificate Authority
    :PROPERTIES:
    :CUSTOM_ID: h-294A4A20
    :END:

**** Letsencrypt
     :PROPERTIES:
     :CUSTOM_ID: h-E1082806
     :END:

Follow [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's instructions]] on setting up letsencrypt along with this yaml snippet below. Replace the hostname where appropriate.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: certmanager-tls-jupyterhub
#+END_SRC

**** Certificate from CA
     :PROPERTIES:
     :CUSTOM_ID: h-205AEDAB
     :END:

Work with sys admin staff to obtain a certificate from a CA.

Follow [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's instructions]] on setting up HTTPS with custom certificates. Note that when adding the key with

#+BEGIN_SRC shell
 kubectl create secret tls <cert-secret> --key ssl.key --cert ssl.crt -n jhub
#+END_SRC

supply the base and intermediate certificates and not the full chain certificate (i.e., with root certificates).

Here is a snippet of what the ingress configuration will look like in the ~secrets.yaml~.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: <secret_name>
#+END_SRC

*** OAuth Authentication
  :PROPERTIES:
  :CUSTOM_ID: h-8A3C5434
  :END:

**** Globus
  :PROPERTIES:
  :CUSTOM_ID: h-C0E8193F
  :END:

[[https://developers.globus.org/][Globus OAuth capability]] is available for user authentication. The instructions [[https://github.com/jupyterhub/oauthenticator#globus-setup][here]] are relatively straightforward.

#+BEGIN_SRC yaml
  auth:
    type: globus
    globus:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
      identityProvider: "xsede.org"
    admin:
      users:
        - adminuser1
#+END_SRC

**** GitHub
     :PROPERTIES:
     :CUSTOM_ID: h-BB3C66CD
     :END:

Setup an OAuth app on GitHub

#+BEGIN_SRC yaml
  auth:
    type: github
    github:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
    admin:
      users:
        - adminuser1
#+END_SRC
*** unidata/unidatahub
    :PROPERTIES:
    :CUSTOM_ID: h-214D1D4C
    :END:

Add the Unidata JupyterHub configuration (~unidata/unidatahub~) and related items (e.g., pulling of Unidata Python projects). Customize the desired CPU / RAM usage. [[https://docs.google.com/spreadsheets/d/15qngBz4L5gwv_JX9HlHsD4iT25Odam09qG3JzNNbdl8/edit?usp=sharing][This spreadsheet]] will help you determine the size of the cluster based on number of users, desired cpu/user, desired RAM/user. Duplicate it and adjust it for your purposes.

#+INCLUDE: "../../../vms/jupyter/example_secrets.yaml" src yaml :lines "21-60"

** Navigate to JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-209E2FBC
   :END:

In a web browser, navigate to your newly minted JupyterHub and see if it is as you expect.

** Tearing Down JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-1E027567
   :END:

*** Total Destructive Tear Down
    :PROPERTIES:
    :CUSTOM_ID: h-A69ADD92
    :END:

Tearing down the JupyterHub including user OpenStack volumes is possible. From the Helm and Kubernetes client:

#+BEGIN_SRC sh
  helm uninstall jhub -n jhub
  kubectl delete namespace jhub
#+END_SRC

To further tear down the Kubernetes cluster see [[file:../../openstack/readme.org::#h-DABDACC7][Tearing Down the Cluster]].

*** Tear Down While Preserving User Volumes
    :PROPERTIES:
    :CUSTOM_ID: h-5F2AA05F
    :END:

A gentler tear down that preserves the user volumes is described in [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's documentation]]. See the section on "persistence of user data".
** Troubleshooting
   :PROPERTIES:
   :CUSTOM_ID: h-0E48EFE9
   :END:
*** Unresponsive JupyterHub
    :PROPERTIES:
    :CUSTOM_ID: h-FF4348F8
    :END:

**** Preliminary Work
     :PROPERTIES:
     :CUSTOM_ID: h-C2429D6E
     :END:

If a JupyterHub becomes unresponsive (e.g., 504 Gateway Time-out), login in to the Kubernetes client and do preliminary backup work in case things go badly. First:

#+BEGIN_SRC shell
  kubectl get pvc -n jhub -o yaml > pvc.yaml.ro
  kubectl get pv -n jhub -o yaml > pv.yaml.ro
  chmod 400 pvc.yaml.ro pv.yaml.ro
#+END_SRC

Make ~pvc.yaml.ro~ ~pv.yaml.ro~ read only since these files could become precious in  case you have to do data recovery for users. More on this subject below.

**** Delete jhub Pods
     :PROPERTIES:
     :CUSTOM_ID: h-6404011E
     :END:

Next, start investigating by issuing:

#+BEGIN_SRC shell
  kubectl get pods -n jhub
#+END_SRC

this command will yield something like

#+BEGIN_SRC shell
  NAME                      READY   STATUS    RESTARTS   AGE
  hub-5bdccd4784-lzw87      1/1     Running   0          17h
  jupyter-joe               1/1     Running   0          4h51m
  proxy-7b986cdb75-mhl86    1/1     Running   0          29d
#+END_SRC

Now start deleting the ~jhub~ pods starting with the user pods (e.g., ~jupyter-joe~).

#+BEGIN_SRC
  kubectl delete pod <pod name> -n jhub
#+END_SRC

Check to see if the JupyterHub is reachable. If it is not, keep deleting pods checking for reachability after each pod deletion.

**** Delete jhub, But Do Not Purge Namespace
     :PROPERTIES:
     :CUSTOM_ID: h-1C4D98E6
     :END:

If the JupyterHub is still not reachable, you can try deleting and recreating the JupyterHub but *do not* delete the namespace as you will wipe out user data.

#+BEGIN_SRC shell
  helm uninstall jhub -n jhub
  # But DO NOT issue this command
  # kubectl delete namespace jhub
#+END_SRC

Then try reinstalling with

#+BEGIN_SRC
  bash install_jhub.sh
#+END_SRC

Now, try recover user volumes as [[https://zonca.dev/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html#delete-and-recreate-openstack-instances][described at the end of the section here]] with the ~pvc.yaml.ro~ ~pv.yaml.ro~ saved earlier (make writable copies of those ~ro~ files). If that still does not work, you can try destroying the entire cluster and recreating it as described in that same link.

*** Volumes Stuck in Reserved State
    :PROPERTIES:
    :CUSTOM_ID: h-354DE174
    :END:

Occasionally, when logging into a JupyterHub the user will encounter a volume attachment error that causes a failure in the login process. The user will see an error that looks something like:

#+BEGIN_SRC shell
2020-03-27 17:54:51+00:00 [Warning] AttachVolume.Attach failed for volume "pvc-5ce953e4-6ad9-11ea-a62a-fa163ebb95dd" : Volume "0349603a-967b-44e2-98d1-0ba1d42c37d8" is attaching, can't finish within the alloted time
#+END_SRC

When you then do an ~openstack volume list~, you will see something like this where a volume is stuck in "reserved":

#+BEGIN_SRC shell
+--------------------------------------+-------------------------------------------------------------+-----------+
| ID                                   | Name                                                        | Status    |
+--------------------------------------+-------------------------------------------------------------+-----------+
| 25c25c5d-75cb-48fd-a9c4-4fd680bea79b | kubernetes-dynamic-pvc-41d76080-6ad7-11ea-a62a-fa163ebb95dd | reserved  |
#+END_SRC

You (or if you do not have permission, Jetstream staff) can reset the volume with:

#+BEGIN_SRC shell
  openstack volume set --state available <uuid>
#+END_SRC

or with

#+BEGIN_SRC shell
      openstack volume list | grep -i reserved | awk \
          'BEGIN { FS = "|" } ; { print $2 }' | xargs -n1 openstack volume set \
      --state available
#+END_SRC

The problem is that once a volume gets stuck like this, it tends to happen again and again. In this scenario, to provide a long term solution to the user, you have to delete their account and associated volume, and recreate their account. Described below are the steps to achieve that:

**** Map Username to Volume UUID and PVC Name
     :PROPERTIES:
     :CUSTOM_ID: h-2DBB1F2F
     :END:

Save information about the username, volume UUID, and persistent volume claim:

#+BEGIN_SRC shell
  kubectl get pvc -n jhub -o yaml > pvc.yaml
#+END_SRC

Scrutinizing this ~yaml~ file, you will easily be able to establish a mapping between the user name, the volume UUID, and the PVC name.

**** Temporarily Reset Volume so User Can Recover Their Work
     :PROPERTIES:
     :CUSTOM_ID: h-C1802F11
     :END:

Reset the volume so that the user can login to the JupyterHub and save or download their work to their local laptop. You know the UUID of the volume associated with the user from the previous step. Remember, this is just a temporary solution since the user's volume will just get stuck again.

#+BEGIN_SRC shell
  openstack volume set --state available <uuid>
#+END_SRC

**** Delete User and PVC
     :PROPERTIES:
     :CUSTOM_ID: h-0730520C
     :END:

***** Delete User 
      :PROPERTIES:
      :CUSTOM_ID: h-9DC5C5C4
      :END:

Once the user has saved their work (see previous steps), via the JupyterHub admin interface, delete the user.

***** Delete PVC Associated with User
      :PROPERTIES:
      :CUSTOM_ID: h-EE5A7663
      :END:

Find the PVC associated with the user:

#+BEGIN_SRC shell
  kubectl get pvc --namespace=jhub
#+END_SRC

which will yield something like:

#+BEGIN_SRC shell
  NAME                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
  claim-user1            Bound    pvc-33202421-dcc3-4933-a992-31987a985e60   10Gi       RWO            standard       5d17h
  claim-user2            Bound    pvc-3a46ca57-5daa-48c3-a5ed-f4dce619dc43   10Gi       RWO            standard       20d
  claim-user3            Bound    pvc-e04b9e5f-9050-4032-aad6-ba0595535d4a   10Gi       RWO            standard       21d
#+END_SRC

Next, delete the PVC associated with the user:

#+BEGIN_SRC shell
  kubectl --namespace=jhub delete pvc claim-<user>
#+END_SRC

**** Recreate User
     :PROPERTIES:
     :CUSTOM_ID: h-B745DD2A
     :END:

Login to the JupyterHub and recreate the user via the JupyterHub admin interface, then have the user login again. They will have to upload the work they saved previously to the JupyterHub.
