#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: auto-id:t
#+TITLE: Creating  a JupyterHub on Jetstream with the Zero to JupyterHub Project
#+DATE:  <2017-06-26 Mon>
#+AUTHOR: Julien Chastang
#+EMAIL: chastang@ucar.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.2 (Org mode 9.0.5)

* Creating  a JupyterHub on Jetstream with the Zero to JupyterHub Project
  :PROPERTIES:
  :CUSTOM_ID: h-D73CBC56
  :END:

** Kubernetes Cluster
   :PROPERTIES:
   :CUSTOM_ID: h-65F9358E
   :END:

*** jupyterhub.sh
    :PROPERTIES:
    :CUSTOM_ID: h-B56E19AB
    :END:

~jupyterhub.sh~ and the related ~z2j.sh~ are convenience scripts similar to ~openstack.sh~ to give you access to a pre-configured environment that will allow you to build and/or run a Zero to JupyterHub cluster. It also relies on the [[file:../../openstack/readme.org::#h-4A9632CC][same Docker container]] as the ~openstack.sh~ script. ~jupyterhub.sh~ takes one argument with the ~-n~ option, the name of the Zero to JupyterHub cluster.  Invoke it from the =science-gateway/openstack= directory. ~jupyterhub.sh~ and the related ~z2j.sh~ ensure the information for this Zero to JupyterHub cluster is persisted outside the container via Docker file mounts -- otherwise all the information about this cluster  would be confined in memory inside the Docker container. The vital information will be persisted in a local =jhub= directory.

*** Create Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-2FF65549
    :END:

[[file:../../openstack/readme.org::#h-DA34BC11][Create a Kubernetes cluster]] with the desired number of nodes and VM sizes. Lock down the master node of the cluster per Unidata security procedures. Work with sys admin staff to obtain a DNS name (e.g., jupyterhub.unidata.ucar.edu), and a certificate from a certificate authority for the master node.

** unidata/unidatahub Docker Container
   :PROPERTIES:
   :CUSTOM_ID: h-CD007D2A
   :END:

Build the Docker container in this directory and push it to dockerhub.

#+BEGIN_SRC sh
  docker build -t unidata/unidatahub:`openssl rand -hex 6` . > /tmp/docker.out 2>&1 &
  docker push unidata/unidatahub:<container id>
#+END_SRC

** Configure and Deploy the JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-E5CA5D99
   :END:

From the client host where you created the Kubernetes cluster, follow [[https://zonca.dev/2020/06/kubernetes-jetstream-kubespray.html#install-jupyterhub][Andrea Zonca's instructions]].

After you have created the ~secrets.yaml~ as instructed, customize it with the choices below

*** Letsencrypt versus Certificate from a Certificate Authority
    :PROPERTIES:
    :CUSTOM_ID: h-294A4A20
    :END:

**** Letsencrypt
     :PROPERTIES:
     :CUSTOM_ID: h-E1082806
     :END:

Follow [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's instructions]] on setting up letsencrypt along with this yaml snippet below. Replace the hostname where appropriate.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: certmanager-tls-jupyterhub
#+END_SRC

**** Certificate from CA
     :PROPERTIES:
     :CUSTOM_ID: h-205AEDAB
     :END:

Work with sys admin staff to obtain a certificate from a CA.

Follow [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's instructions]] on setting up HTTPS with custom certificates. Note that when adding the key with

#+BEGIN_SRC shell
 kubectl create secret tls <cert-secret> --key ssl.key --cert ssl.crt -n jhub
#+END_SRC

supply the base and intermediate certificates and not the full chain certificate (i.e., with root certificates).

Here is a snippet of what the ingress configuration will look like in the ~secrets.yaml~.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: <secret_name>
#+END_SRC

*** OAuth Authentication
  :PROPERTIES:
  :CUSTOM_ID: h-8A3C5434
  :END:

**** Globus
  :PROPERTIES:
  :CUSTOM_ID: h-C0E8193F
  :END:

[[https://developers.globus.org/][Globus OAuth capability]] is available for user authentication. The instructions [[https://github.com/jupyterhub/oauthenticator#globus-setup][here]] are relatively straightforward.

#+BEGIN_SRC yaml
  auth:
    type: globus
    globus:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
      identityProvider: "xsede.org"
    admin:
      users:
        - adminuser1
#+END_SRC

**** GitHub
     :PROPERTIES:
     :CUSTOM_ID: h-BB3C66CD
     :END:

Setup an OAuth app on GitHub

#+BEGIN_SRC yaml
  auth:
    type: github
    github:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
    admin:
      users:
        - adminuser1
#+END_SRC
*** unidata/unidatahub
    :PROPERTIES:
    :CUSTOM_ID: h-214D1D4C
    :END:

Add the Unidata JupyterHub configuration (~unidata/unidatahub~) and related items (e.g., pulling of Unidata Python projects). Customize the desired CPU / RAM usage. [[https://docs.google.com/spreadsheets/d/15qngBz4L5gwv_JX9HlHsD4iT25Odam09qG3JzNNbdl8/edit?usp=sharing][This spreadsheet]] will help you determine the size of the cluster based on number of users, desired cpu/user, desired RAM/user. Duplicate it and adjust it for your purposes.

#+INCLUDE: "../../../vms/jupyter/example_secrets.yaml" src yaml :lines "21-60"

** Navigate to JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-209E2FBC
   :END:

In a web browser, navigate to your newly minted JupyterHub and see if it is as you expect.

** Tearing Down JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-1E027567
   :END:

*** Total Destructive Tear Down
    :PROPERTIES:
    :CUSTOM_ID: h-A69ADD92
    :END:

Tearing down the JupyterHub including user OpenStack volumes is possible. From the Helm and Kubernetes client:

#+BEGIN_SRC sh
  helm uninstall jhub -n jhub
  # Think before you type !
  echo $CLUSTER; sleep 60; kubectl delete namespace jhub
#+END_SRC

To further tear down the Kubernetes cluster see [[file:../../openstack/readme.org::#h-DABDACC7][Tearing Down the Cluster]].

*** Tear Down While Preserving User Volumes
    :PROPERTIES:
    :CUSTOM_ID: h-5F2AA05F
    :END:

A gentler tear down that preserves the user volumes is described in [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's documentation]]. See the section on "persistence of user data".
** Troubleshooting
   :PROPERTIES:
   :CUSTOM_ID: h-0E48EFE9
   :END:
*** Unresponsive JupyterHub
    :PROPERTIES:
    :CUSTOM_ID: h-FF4348F8
    :END:

**** Preliminary Work
     :PROPERTIES:
     :CUSTOM_ID: h-C2429D6E
     :END:

If a JupyterHub becomes unresponsive (e.g., 504 Gateway Time-out), login in to the Kubernetes client and do preliminary backup work in case things go badly. First:

#+BEGIN_SRC shell
  kubectl get pvc -n jhub -o yaml > pvc.yaml.ro
  kubectl get pv -n jhub -o yaml > pv.yaml.ro
  chmod 400 pvc.yaml.ro pv.yaml.ro
#+END_SRC

Make ~pvc.yaml.ro~ ~pv.yaml.ro~ read only since these files could become precious in  case you have to do data recovery for users. More on this subject below.

**** Delete jhub Pods
     :PROPERTIES:
     :CUSTOM_ID: h-6404011E
     :END:

Next, start investigating by issuing:

#+BEGIN_SRC shell
  kubectl get pods -n jhub
#+END_SRC

this command will yield something like

#+BEGIN_SRC shell
  NAME                      READY   STATUS    RESTARTS   AGE
  hub-5bdccd4784-lzw87      1/1     Running   0          17h
  jupyter-joe               1/1     Running   0          4h51m
  proxy-7b986cdb75-mhl86    1/1     Running   0          29d
#+END_SRC

Now start deleting the ~jhub~ pods starting with the user pods (e.g., ~jupyter-joe~).

#+BEGIN_SRC
  kubectl delete pod <pod name> -n jhub
#+END_SRC

Check to see if the JupyterHub is reachable. If it is not, keep deleting pods checking for reachability after each pod deletion.

**** Delete jhub, But Do Not Purge Namespace
     :PROPERTIES:
     :CUSTOM_ID: h-1C4D98E6
     :END:

If the JupyterHub is still not reachable, you can try deleting and recreating the JupyterHub but *do not* delete the namespace as you will wipe out user data.

#+BEGIN_SRC shell
  helm uninstall jhub -n jhub
  # But DO NOT issue this command
  # kubectl delete namespace jhub
#+END_SRC

Then try reinstalling with

#+BEGIN_SRC
  bash install_jhub.sh
#+END_SRC

Now, try recover user volumes as [[https://zonca.dev/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html#delete-and-recreate-openstack-instances][described at the end of the section here]] with the ~pvc.yaml.ro~ ~pv.yaml.ro~ saved earlier (make writable copies of those ~ro~ files). If that still does not work, you can try destroying the entire cluster and recreating it as described in that same link.

*** Volumes Stuck in Reserved State
    :PROPERTIES:
    :CUSTOM_ID: h-354DE174
    :END:

**** Background
     :PROPERTIES:
     :CUSTOM_ID: h-1765D7EB
     :END:

Occasionally, when logging into a JupyterHub the user will encounter a volume attachment error that causes a failure in the login process. [[https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/issues/40][This is an ongoing issue on Jetstream that we have never been able to get to the bottom of]]. The user will see an error that looks something like:

#+BEGIN_SRC shell
2020-03-27 17:54:51+00:00 [Warning] AttachVolume.Attach failed for volume "pvc-5ce953e4-6ad9-11ea-a62a-fa163ebb95dd" : Volume "0349603a-967b-44e2-98d1-0ba1d42c37d8" is attaching, can't finish within the alloted time
#+END_SRC

When you then do an ~openstack volume list~, you will see something like this where a volume is stuck in "reserved":

#+BEGIN_SRC shell
|--------------------------------------+------------------------------------------+----------|
| ID                                   | Name                                     | Status   |
|--------------------------------------+------------------------------------------+----------|
| 25c25c5d-75cb-48fd-a9c4-4fd680bea79b | pvc-41d76080-6ad7-11ea-a62a-fa163ebb95dd | reserved |
|--------------------------------------+------------------------------------------+----------|
#+END_SRC

You (or if you do not have permission, Jetstream staff) can reset the volume with:

#+BEGIN_SRC shell
  openstack volume set --state available <volume uuid>
#+END_SRC

or with

#+BEGIN_SRC shell
      openstack volume list | grep -i reserved | awk \
          'BEGIN { FS = "|" } ; { print $2 }' | xargs -n1 openstack volume set \
      --state available
#+END_SRC

The problem is that once a volume gets stuck like this, it tends to happen again and again. In this scenario, to provide a long term solution to the user, you have to save their data, delete their account, associated PVC (persistent volume claim), recreate their account, and restore their data. I describe below the steps to achieve that objective:

**** Map Username to Volume UUID and PVC Name
     :PROPERTIES:
     :CUSTOM_ID: h-2DBB1F2F
     :END:

Obtain information about the username, volume UUID, and PVC:

#+BEGIN_SRC shell
  kubectl get pvc --namespace=jhub | grep <name of volume obtained in the last section>
#+END_SRC

for example

#+BEGIN_SRC shell
  kubectl get pvc --namespace=jhub | grep pvc-3e4032f8-3f8f-4e20-801a-a6a322175c9a
#+END_SRC

will yield something like:

#+BEGIN_SRC shell
  claim-<user>             Bound    pvc-3e4032f8-3f8f-4e20-801a-a6a322175c9a   10Gi       RWO            standard       61m
#+END_SRC

Now you know the user, volume UUID, and PVC associated with the problematic volume.

**** Reset User Volume
     :PROPERTIES:
     :CUSTOM_ID: h-42C693B9
     :END:

You can now reset the volume as a short term fix, but you still will have to provide a longer term solution to the user (keep reading below).

#+BEGIN_SRC shell
  openstack volume set --state available <uuid>
#+END_SRC

**** Save User Data
     :PROPERTIES:
     :CUSTOM_ID: h-C1802F11
     :END:

Next, from the OpenStack command line, attach the volume to a VM so you can recover their work. (Sometimes you have to wait until the student logs off the JupyterHub and you may have to reset the volume again.)

#+BEGIN_SRC shell
  openstack server add volume <vm-uid-number> <volume-uid-number>
#+END_SRC

Now login to the recovery VM and do something like the next command. I've assumed the device (e.g., =/dev/sdb=) and data recovery directory (e.g., ~sudo mkdir /data-jh~):
#+BEGIN_SRC shell
  sudo mount /dev/sdb /data-jh
#+END_SRC

create a directory and use ~rsync~ to grab user data:

#+BEGIN_SRC shell
  mkdir user; cd user
  sudo rsync -rt --progress /data-jh/ .
#+END_SRC

The user data has now been saved (though *verify* this is true). Next ~umount~:

#+BEGIN_SRC shell
  sudo umount /dev/sdb
#+END_SRC

detach VM from the OpenStack command line:

#+BEGIN_SRC shell
  openstack server remove volume <vm-uid-number> <volume-uid-number>
#+END_SRC

**** Delete JupyterHub User
     :PROPERTIES:
     :CUSTOM_ID: h-9DC5C5C4
     :END:

*This is a step where you want to think before you act!* Once you have saved the user's work (see previous steps) via the JupyterHub admin interface, delete the user.

**** Delete PVC Associated with User
     :PROPERTIES:
     :CUSTOM_ID: h-EE5A7663
     :END:

Next, delete the PVC associated with the user:

#+BEGIN_SRC shell
  kubectl --namespace=jhub delete pvc claim-<user>
#+END_SRC

You have to do this step, or else the recreated user will be attached to the same PVC and the problem will happen again.

**** Recreate User
     :PROPERTIES:
     :CUSTOM_ID: h-B745DD2A
     :END:

Login to the JupyterHub admin interface and recreate the user in question. Also, start and shutoff their server so that they have a fresh volume where you will restore their data.

**** Recover User Data
     :PROPERTIES:
     :CUSTOM_ID: h-4670F9D3
     :END:

You now have to do the reverse of some of the steps we just described. Discover user's new volume:

 #+BEGIN_SRC shell
  kubectl get pvc --namespace=jhub | grep <user>
#+END_SRC

note the volume name (e.g., ~pvc-41d76080-6ad7-11ea-a62a-fa163ebb95dd~). Use that name to find the actual volume:

 #+BEGIN_SRC shell
  openstack volume list | grep <pvc-name>
#+END_SRC

which will give you the volume UUID. At this point, you have the volume ID, and you have to do what you did earlier in reverse:

#+BEGIN_SRC shell
  openstack server add volume <vm-uid-number> <volume-uuid-number>
#+END_SRC

login to recovery VM and:

#+BEGIN_SRC shell
  sudo mount /dev/sdb /data-jh
#+END_SRC

Clear out any files that are in =/data-jh=. Again, think before typing here. ~cd~ to the directory where you saved the user's data earlier. ~rsync~ user data back onto their new volume:

#+BEGIN_SRC shell
  sudo rsync -rt --progress .  /data-jh/
#+END_SRC

~chown~ for good measure:

#+BEGIN_SRC shell
  cd /data-jh/
  sudo chown -R ubuntu:ubuntu .
#+END_SRC

Check to ensure the user data has indeed been recovered. Next, ~umount~ again:

#+BEGIN_SRC shell
  sudo umount /dev/sdb
#+END_SRC

from the OpenStack command line:

#+BEGIN_SRC shell
  openstack server remove volume <vm-uid-number> <volume-uuid-number>
#+END_SRC

You are finally done. Next time the user logs in to their JupyterHub, they will be on a new PVC/Volume and hopefully this problem should not happen again for that user.

**** Script to Mitigate Problem
     :PROPERTIES:
     :CUSTOM_ID: h-F7B1FC52
     :END:

Invoking this script (e.g., call it ~notify.sh~) from crontab, maybe every three minutes or so, can help mitigate the problem and give you faster notification of the issue. Note [[https://ifttt.com][iftt]] is a push notification service with webhooks available that can notify your smart phone triggered by a ~curl~ invocation as demonstrated below. You'll have to create an ifttt login and download the app on your smart phone.

#+BEGIN_SRC shell
  #!/bin/bash

  source /home/centos/.bash_profile

  VAR=$(openstack volume list -f value -c ID -c Status | grep -i reserved | wc -l)

  MSG="Subject: Volume Stuck in Reserved on Jetstream"

  if [[ $VAR -gt 0 ]]
  then
      echo $MSG | /usr/sbin/sendmail my@email.com
      openstack volume list | grep -i reserved >> /tmp/stuck.txt
      curl -X POST https://maker.ifttt.com/trigger/jetstream/with/key/xyz
      openstack volume list -f value -c ID -c Status | grep -i reserved | awk \
          '{ print $1 }' | xargs -n1 openstack volume set --state available
  fi
#+END_SRC

you can invoke this script from crontab:

#+BEGIN_SRC shell
  */3 * * * * /home/centos/notify.bash > /dev/null 2>&1
#+END_SRC

Note, again, this is just a temporary solution. You still have to provide a longer-term solution as described earlier in this section.

**** Easier Way
     :PROPERTIES:
     :CUSTOM_ID: h-C7828F07
     :END:

In the summer / fall of 2021, GitHub user [[https://github.com/freedge][@freedge]] [[https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/issues/40][proposed]] a much easier way to recover from this problem. Specifically, they suggested checking the status of the volume attachment in the cinder attachment list. Here is an example:

#+BEGIN_SRC shell
  cinder attachment-list  | grep -i d910c7fae38b
  WARNING:cinderclient.shell:API version 3.62 requested,
  WARNING:cinderclient.shell:downgrading to 3.55 based on server support.
  | 67dbf5c3-c190-4f9e-a2c9-78da44df6c75 | cf1a7adf-7b0a-422f-8843-d910c7fae38b | reserved  | 0593faaf-8ba0-4eb5-84ad-b7282ce5aac2 |
#+END_SRC

At this point, you may see two entries (even though only one is shown here). One attachment in reserved and one that is attached.

Next, delete the reserved attachment:

#+BEGIN_SRC shell
  cinder attachment-delete 67dbf5c3-c190-4f9e-a2c9-78da44df6c75
#+END_SRC

This solution seems to address the issue in that the volume is no longer stuck in reserved, so we don't have to come up with as many convoluted workarounds (see above) to address this bug. There is still an underlying issue here, but at least this approach is much easier to implement.
