#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: auto-id:t
#+TITLE: Creating a JupyterHub on Jetstream with the Zero to JupyterHub Project
#+DATE:  <2017-06-26 Mon>
#+AUTHOR: Julien Chastang
#+EMAIL: chastang@ucar.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.2 (Org mode 9.0.5)
#+STARTUP: content

* Creating a JupyterHub on Jetstream with the Zero to JupyterHub Project
  :PROPERTIES:
  :CUSTOM_ID: h-D73CBC56
  :END:

** Kubernetes Cluster
   :PROPERTIES:
   :CUSTOM_ID: h-65F9358E
   :END:

*** jupyterhub.sh
    :PROPERTIES:
    :CUSTOM_ID: h-B56E19AB
    :END:

~jupyterhub.sh~ and the related ~z2j.sh~ are convenience scripts similar to ~openstack.sh~ to give you access to a pre-configured environment that will allow you to build and/or run a Zero to JupyterHub cluster. It also relies on the [[../../openstack/readme.md][same Docker container]] as the ~openstack.sh~ script. ~jupyterhub.sh~ takes the following
required arguments:

#+BEGIN_SRC shell
-n, --name JupyterHub name
-p, --ip JupyterHub IP
-o, --openrc openrc.sh absolute path
#+END_SRC

/Important/: The ~--name~ argument is used to set the names of the instances (VMs) of the cluster, which in turn is used to define the DNS name of assigned to the floating IP of the master node ([[../../vms/openstack/readme.org::#h-612458CB][see here]]). Ensure that the name provided to ~jupyterhub.sh~ results in a domain name that is less than 64 characters long, else LetsEncrypt will not be able to issue a certificate ([[https://letsencrypt.org/docs/glossary/#def-CN][see here]]).

Invoke ~jupyterhub.sh~ from the ~science-gateway/openstack~ directory. ~jupyterhub.sh~ and the related ~z2j.sh~ ensure the information for this Zero to JupyterHub cluster is persisted outside the container via Docker file mounts -- otherwise all the information about this cluster would be confined in memory inside the Docker container. The vital information will be persisted in a local ~jhub~ directory.

*** Create Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-2FF65549
    :END:

[[../../openstack/readme.md][Create a Kubernetes cluster]] with the desired number of nodes and VM sizes. Lock down the master node of the cluster per Unidata security procedures. Work with sys admin staff to obtain a DNS name (e.g., jupyterhub.unidata.ucar.edu), and a certificate from a certificate authority for the master node. Alternatively, you can
use JetStream2's [[../../openstack/readme.md#dynamicdns][dynamic DNS]] and acquire a self signed certificate with [[#h-294A4A20][LetsEncrypt]].

** Docker Image for JupyterHub User Environment
   :PROPERTIES:
   :CUSTOM_ID: h-CD007D2A
   :END:

Build the Docker container that will be employed by the user environment on their JupyterHub instance. This Docker image will be [[#h-214D1D4C][referenced in the secrets.yaml]]. Uniquely tag the image with a date and ID for sane retrieval and referencing. For example:

#+BEGIN_SRC sh
  docker build -t unidata/unidatahub:`date +%Y%b%d_%H%M%S`_`openssl rand -hex 4` . > /tmp/docker.out 2>&1 &
  docker push unidata/unidatahub:<container id>
#+END_SRC

** Configure and Deploy the JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-E5CA5D99
   :END:

From the client host where you created the Kubernetes cluster, follow [[https://zonca.dev/2020/06/kubernetes-jetstream-kubespray.html#install-jupyterhub][Andrea Zonca's instructions]].

After you have created the ~secrets.yaml~ as instructed, customize it with the choices below

*** SSL Certificates
    :PROPERTIES:
    :CUSTOM_ID: h-294A4A20
    :END:

**** Letsencrypt
     :PROPERTIES:
     :CUSTOM_ID: h-E1082806
     :END:

Follow [[https://www.zonca.dev/posts/2020-03-13-setup-https-kubernetes-letsencrypt.html][Andrea's instructions]] on setting up letsencrypt using [[https://cert-manager.io/][cert-manager]]. Due to a [[https://docs.jetstream-cloud.org/faq/trouble/#i-cant-ping-or-reach-a-publicfloating-ip-from-an-internal-non-routed-host][network change between JS1 and JS2]], the cert-manager pods must be run on the k8s master node in order to successfully complete the [[https://letsencrypt.org/how-it-works/][challenges]] required by letsencrypt to issue the certificate. Pay special attention to the [[https://www.zonca.dev/posts/2020-03-13-setup-https-kubernetes-letsencrypt.html#bind-the-pods-to-the-master-node][Bind the pods to the master node]] section.

For further reading:

   - [[https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/#create-a-pod-that-gets-scheduled-to-your-chosen-node][Assigning a pod to a specific node]]
   - [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][Taints and Tolerations]]

**** Certificate from Certificate Authority
     :PROPERTIES:
     :CUSTOM_ID: h-205AEDAB
     :END:

Work with Unidata system administrator staff to obtain a certificate from a trusted certificate authority.

Follow [[https://www.zonca.dev/posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub#setup-https-with-custom-certificates][Andrea's instructions]] on setting up HTTPS with custom certificates. Note that when adding the key with

#+BEGIN_SRC shell
 kubectl create secret tls <cert-secret> --key ssl.key --cert ssl.crt -n jhub
#+END_SRC

supply the base and intermediate certificates and not the full chain certificate (i.e., with root certificates). You can find these certificates [[https://uit.stanford.edu/service/ssl/chain][here]].

Here is a snippet of what the ingress configuration will look like in the ~secrets.yaml~.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      cert-manager.io/issuer: "incommon"
    hosts:
        - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: <secret_name>
#+END_SRC

***** Certificate Expiration and Renewal
:PROPERTIES:
:CUSTOM_ID: h-055BCE98
:END:

When these certificates expire, they can be updated with the snippet below, but *be careful* to update the certificate on the correct JupyterHub deployment. Otherwise, you will be in cert-manger hell.

#+BEGIN_SRC shell
kubectl create secret tls cert-secret --key ssl.key --cert ssl.crt -n jhub \
    --dry-run=client -o yaml | kubectl apply -f -
#+END_SRC

*** OAuth Authentication
  :PROPERTIES:
  :CUSTOM_ID: h-8A3C5434
  :END:

**** Globus
  :PROPERTIES:
  :CUSTOM_ID: h-C0E8193F
  :END:

[[https://developers.globus.org/][Globus OAuth capability]] is available for user authentication. The instructions [[https://oauthenticator.readthedocs.io/en/latest/reference/api/gen/oauthenticator.globus.html][here]] are relatively straightforward.

#+BEGIN_SRC yaml
  auth:
    type: globus
    globus:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
      identityProvider: "xsede.org"
    admin:
      users:
        - adminuser1
#+END_SRC

**** GitHub
     :PROPERTIES:
     :CUSTOM_ID: h-BB3C66CD
     :END:

Setup an OAuth app on GitHub

#+BEGIN_SRC yaml
  auth:
    type: github
    github:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
    admin:
      users:
        - adminuser1
#+END_SRC

*** Docker Image and Other Configuration
    :PROPERTIES:
    :CUSTOM_ID: h-214D1D4C
    :END:

Reference [[#h-CD007D2A][the previously built Docker image]] (e.g., ~unidata/unidatahub:2022Dec15_031132_fe2ea584~). Customize the desired CPU / RAM usage. [[https://docs.google.com/spreadsheets/d/15qngBz4L5gwv_JX9HlHsD4iT25Odam09qG3JzNNbdl8/edit?usp=sharing][This spreadsheet]] will help you determine the size of the cluster based on number of users, desired cpu/user, desired RAM/user. Duplicate it and adjust it for your purposes.

#+INCLUDE: "../../../vms/jupyter/secrets.yaml" src yaml :lines "35-62"

*** JupyterHub Profiles
:PROPERTIES:
:CUSTOM_ID: h-5BE09B80
:END:

A JupyterHub may be configured to give users different [[https://z2jh.jupyter.org/en/stable/jupyterhub/customizing/user-environment.html#using-multiple-profiles-to-let-users-select-their-environment][profile options]] when logging in. This can be useful when, for example, a faculty member is using JupyterHub for multiple courses and wants to keep them seperate. Another use case is for creating "high power" or "low power" environments, which are allocated varying levels of computational resources, i.e. RAM and CPU. This can be applied in an undergraduate research setting where an instructor and their students use the low power environments during synchronous instruction and the high power environment for asynchronous workflows.

An example of high and low power environments is shown below.

#+begin_src yaml
  singleuser:
    # Set defaults and options shared by all profiles
    extraEnv:
      NBGITPULLER_DEPTH: "0"
    storage:
      capacity: 5Gi
    startTimeout: 600
    image:
      name: "unidata/someImage"
      tag: "someTag"
    # Profile definitions
    profileList:
      - display_name: "High Power (default)"
        description: "12 GB of memory; up to 4 vCPUs"
        kubespawner_override:
          mem_guarantee: 12G
          mem_limit: 12G
          cpu_guarantee: 2
          cpu_limit: 4
        default: true
      - display_name: "Low Power"
        description: "6 GB of memory; up to 2 vCPUS"
        kubespawner_override:
          mem_guarantee: 6G
          mem_limit: 6G
          cpu_guarantee: 1
          cpu_limit: 2
#+end_src

Note, however, that while one would typically provide ~secrets.yaml~ with the CPU and memory guarantees/limits as shown below, when using the ~kubespawner_override~ object to set these options for various profiles, you must provide the names of the fields as Kubespawner will recognize them.

#+begin_src yaml
# Typical manner of configuring CPU and memory options
singleuser:
  memory:
    guarantee: 4G
    limit: 4G
  cpu:
    guarantee: 1
    limit: 2

# Kubespawner override
singleuser:
  profileList:
    - kubespawner_override:
      mem_guarantee: 16G
      mem_limit: 16G
      cpu_guarantee: 4
      cpu_limit: 4
#+end_src

See [[https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/1242#issuecomment-484895216][this]] GitHub issue for a description of the discrepancy, and the [[https://jupyterhub-kubespawner.readthedocs.io/en/latest/spawner.html][Kubespawner docs]] for the appropriate names to use for the various options when creating profiles.

*** Create a Large Data Directory That Can Be Shared Among All Users
:PROPERTIES:
:CUSTOM_ID: h-C95C198A
:END:

[[https://www.zonca.dev/posts/2023-02-06-nfs-server-kubernetes-jetstream][Andrea has a tutorial about sharing a directory]] (e.g., =/share/data=) via Kubernetes and NFS. The instructions basically work as advertised with the KubeSpray option (not Magnum -- I have not tried that), e.g.,

#+begin_src yaml
  nfs:
      # for Magnum
      # server: 10.254.204.67
      # for Kubespray
      server: 10.233.46.63
      path: /
#+end_src

The ~clusterIP~ is arbitrary and the one in the =jupyterhub-deploy-kubernetes-jetstream/nfs/= directory works. That IP is referenced in multiple locations in that directory. Make sure you get them all.

Define the size of the shared volume in =create_nfs_volume.yaml=, e.g.,:

#+begin_src yaml
  resources:
    requests:
      storage: 300Gi
#+end_src

Verify ~nfs-common~ is installed on the worker nodes (more recent versions of AZ's ~jetstream_kubespray~ project will have this already so you won't have to manually install the package), e.g.,

#+begin_src sh
  sudo apt install -y nfs-common
#+end_src

*** Ensure "Core" Pods Are Scheduled on a Dedicated Node
:PROPERTIES:
:CUSTOM_ID: h-6784737C
:END:

When a JupyterHub is expected to be used for especially resource intensive tasks, for example running WRF from within JupyterHub, by multiple users simultaneously, their single user pods can use all of a worker node's resources. This is a problem when these worker nodes also contain the JupyterHub's [[https://z2jh.jupyter.org/en/stable/resources/reference.html#scheduling-corepods][core pods]], which all perform some essential function of a healthy Zero-to-JupyterHub cluster. In particular, it's been observed that if the proxy pod, the component which routes both internal and external requests to the Hub and single user servers, does not have the necessary resources, the JupyterHub will crash.

To prevent this from happening, we can ensure all core pods are scheduled on a dedicated node. This is accomplished by assigning to a chosen node a [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][taint]], an attritube which prevents pods from spawning unless they have the corresponding [[https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/][toleration]]. This alone is not enough however, as a pod's [[https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/][node affinity]] must require it to spawn on that specific node. The process is described below.

Add the taint to ~<node-name~:

#+BEGIN_SRC shell
  kubectl taint nodes <node-name> hub.jupyter.org/dedicated=core:NoSchedule
#+END_SRC

Add the label that the pods will look for when being scheduled on a node:

#+BEGIN_SRC shell
  kubectl label nodes <node-name> hub.jupyter.org/node-purpose=core
#+END_SRC

No ~kubectl~ commands need to be explicitly executed to modify the core pods. The toleration is applied to the core pods [[https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/HEAD/jupyterhub/values.yaml#L552][by default]], however add the following to the ~secrets.yaml~ in order to make our intentions explicit. It is also noted that, by default, pods are [[https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/HEAD/jupyterhub/values.yaml#L563][preferred]], not required, to spawn on this dedicated core node. Thus, ensure that ~scheduling.corePods.nodeAffinity.matchNodePurpose~ is set to ~require~.

#+BEGIN_SRC yaml
  scheduling:
    corePods:
      tolerations:
        - key: hub.jupyter.org/dedicated
          operator: Equal
          value: core
          effect: NoSchedule
        - key: hub.jupyter.org_dedicated
          operator: Equal
          value: core
          effect: NoSchedule
      nodeAffinity:
        matchNodePurpose: require
#+END_SRC

After all these changes have been made, run ~bash install_jhub.sh~ once again to apply them, and run a ~kubectl get pods -n jhub -o wide~ to confirm that core pods are running on the intended node. Single user pods should no longer be spawned on the dedicated core node, but any preexisting single user pods will may still reside on this node until they are eventually culled by the Hub.

** Navigate to JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-209E2FBC
   :END:

In a web browser, navigate to your newly minted JupyterHub and see if it is as you expect.

** Tearing Down JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-1E027567
   :END:

*** Total Destructive Tear Down
    :PROPERTIES:
    :CUSTOM_ID: h-A69ADD92
    :END:

Tearing down the JupyterHub including user OpenStack volumes is possible. From the Helm and Kubernetes client:

#+BEGIN_SRC sh
  helm uninstall jhub -n jhub
  # Think before you type !
  echo $CLUSTER; sleep 60; kubectl delete namespace jhub
#+END_SRC

To further tear down the Kubernetes cluster see [[file:../../openstack/readme.org::#h-DABDACC7][Tearing Down the Cluster]].

*** Tear Down While Preserving User Volumes
    :PROPERTIES:
    :CUSTOM_ID: h-5F2AA05F
    :END:

A gentler tear down that preserves the user volumes is described in [[https://www.zonca.dev/posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub][Andrea's documentation]]. See the section on "persistence of user data".
*** Locating and Deleting Orphaned PVCs
:PROPERTIES:
:CUSTOM_ID: h-801B7EE9
:END:
Inevitably, when launching and tearing down numerous JupyterHub clusters, there will be times when Persistent Volume Claims (PVCs) and associated OpenStack volumes are orphaned, i.e., not attached to any JupyterHub cluster. These should be periodically cleaned up to not consume the OpenStack volume allocation.

The following set of commands should be done on the JupyterHub control VM where all the clusters are managed from various docker containers, e.g.,:

#+begin_src sh
  $ docker ps
  CONTAINER ID   IMAGE                         COMMAND       CREATED        STATUS       PORTS     NAMES
  a3b0c55520e9   unidata/science-gateway-gpu   "/bin/bash"   4 weeks ago    Up 4 weeks             unidata-jupyterhub
  12769e052f2e   unidata/science-gateway       "/bin/bash"   3 months ago   Up 3 weeks             mvu-test
  84625056d84d   unidata/science-gateway       "/bin/bash"   3 months ago   Up 4 weeks             ou23s
  ead47ea3eb99   unidata/science-gateway       "/bin/bash"   3 months ago   Up 4 weeks             mvu23s
  196a6308afdb   unidata/science-gateway       "/bin/bash"   3 months ago   Up 4 weeks             fsu-s23
#+end_src

Also make sure that list is accurate and complete, i.e., all JupyterHub clusters currently running are accounted for. Otherwise, you may miss active PVC volumes that you could potentially accidentally delete.

**** Obtain PVCs That Are in Use
:PROPERTIES:
:CUSTOM_ID: h-020D86A3
:END:

This command chain performs several actions to get a list of all Kubernetes Persistent Volume Claims (PVCs) within multiple Docker containers, each managing a JupyterHub cluster. The output is then sorted, cleaned of white space, and saved to a file. The end result is that you have a list of all PVCs that are currently in use.

#+begin_src sh
  docker ps -q | xargs -I {} -n1 docker exec -t {} bash -c \
                       'kubectl get pvc -A | tail -n +2' | \
      awk '{print $4}' | sort | tr -d "[:blank:]" > /tmp/pvc.out
#+end_src

**** Obtain All the OpenStack Volumes
:PROPERTIES:
:CUSTOM_ID: h-0ACAC986
:END:

This command chain performs several actions to get a list of all the OpenStack volume names related to PVCs (active or orphaned) from within a specific Docker container. The output is then sorted, cleaned of blanks, and saved to a file. The ~container_id~ can be chosen from the list of containers described above. It does not matter which one as long as it has access to the OpenStack CLI.

#+begin_src sh
  docker exec -t <container_id> bash -c \
         'source ~/.bashrc && openstack volume list' | grep pvc | \
      awk -F'|' '{print $3}' | sort | tr -d "[:blank:]" > /tmp/pvc-total.out
#+end_src

**** Find the Orphaned Volumes
:PROPERTIES:
:CUSTOM_ID: h-ED8A929F
:END:

Find the orphaned volumes by taking the set difference of the files generated by the last two commands:

#+begin_src sh
  comm -23 /tmp/pvc-total.out /tmp/pvc.out > /tmp/pvc-orphaned.out
#+end_src

*Important*: Scrutinize the contents of =/tmp/pvc-orphaned.out= to ensure they are not, in fact, being used anywhere.

**** Delete Orphaned Volumes
:PROPERTIES:
:CUSTOM_ID: h-D62E010F
:END:

Copy this file into one of the Docker containers listed above. It does not matter which one as long as you have an OpenStack CLI.

#+begin_src sh
      docker ps -q | xargs -I {} -n1 docker cp /tmp/pvc-orphaned.out {}:/tmp/pvc-orphaned.out
#+end_src

You can now delete the orphaned volumes with a script the looks like this. Again, think before you type as you are about to delete a number of OpenStack volumes.

#+begin_src sh
  #!/bin/bash

  # Source your OpenStack credentials
  # source openrc

  # Read the file line by line
  while IFS= read -r volume_name
  do
    # Use the OpenStack CLI to get the volume ID
    volume_id=$(openstack volume show "$volume_name" -f value -c id)
    echo "$volume_name: $volume_id"
    # openstack volume delete $volume_id
  done < /tmp/pvc-orphaned.out
#+end_src

** Troubleshooting
   :PROPERTIES:
   :CUSTOM_ID: h-0E48EFE9
   :END:
*** Unresponsive JupyterHub
    :PROPERTIES:
    :CUSTOM_ID: h-FF4348F8
    :END:

**** Preliminary Work
     :PROPERTIES:
     :CUSTOM_ID: h-C2429D6E
     :END:

If a JupyterHub becomes unresponsive (e.g., 504 Gateway Time-out), login in to the Kubernetes client and do preliminary backup work in case things go badly. First:

#+BEGIN_SRC shell
  kubectl get pvc -n jhub -o yaml > pvc.yaml.ro
  kubectl get pv -n jhub -o yaml > pv.yaml.ro
  chmod 400 pvc.yaml.ro pv.yaml.ro
#+END_SRC

Make ~pvc.yaml.ro~ ~pv.yaml.ro~ read only since these files could become precious in  case you have to do data recovery for users. More on this subject below.

**** Delete jhub Pods
     :PROPERTIES:
     :CUSTOM_ID: h-6404011E
     :END:

Next, start investigating by issuing:

#+BEGIN_SRC shell
  kubectl get pods -o wide -n jhub
#+END_SRC

this command will yield something like

#+BEGIN_SRC shell
  NAME                      READY   STATUS    RESTARTS   AGE
  hub-5bdccd4784-lzw87      1/1     Running   0          17h
  jupyter-joe               1/1     Running   0          4h51m
  proxy-7b986cdb75-mhl86    1/1     Running   0          29d
#+END_SRC

Now start deleting the ~jhub~ pods starting with the user pods (e.g., ~jupyter-joe~).

#+BEGIN_SRC
  kubectl delete pod <pod name> -n jhub
#+END_SRC

Check to see if the JupyterHub is reachable. If it is not, keep deleting pods checking for reachability after each pod deletion.

**** Delete jhub, But Do Not Purge Namespace
     :PROPERTIES:
     :CUSTOM_ID: h-1C4D98E6
     :END:

If the JupyterHub is still not reachable, you can try deleting and recreating the JupyterHub but *do not* delete the namespace as you will wipe out user data.

#+BEGIN_SRC shell
  helm uninstall jhub -n jhub
  # But DO NOT issue this command
  # kubectl delete namespace jhub
#+END_SRC

Then try reinstalling with

#+BEGIN_SRC
  bash install_jhub.sh
#+END_SRC

Now, try recover user volumes as [[https://zonca.dev/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html#delete-and-recreate-openstack-instances][described at the end of the section here]] with the ~pvc.yaml.ro~ ~pv.yaml.ro~ saved earlier (make writable copies of those ~ro~ files). If that still does not work, you can try destroying the entire cluster and recreating it as described in that same link.

*** Volumes Stuck in Reserved State
    :PROPERTIES:
    :CUSTOM_ID: h-354DE174
    :END:

**** Background
     :PROPERTIES:
     :CUSTOM_ID: h-1765D7EB
     :END:

Occasionally, when logging into a JupyterHub the user will encounter a volume attachment error that causes a failure in the login process. [[https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/issues/40][This is an ongoing issue on Jetstream that we have never been able to get to the bottom of]]. The user will see an error that looks something like:

#+BEGIN_SRC shell
2020-03-27 17:54:51+00:00 [Warning] AttachVolume.Attach failed for volume "pvc-5ce953e4-6ad9-11ea-a62a-fa163ebb95dd" : Volume "0349603a-967b-44e2-98d1-0ba1d42c37d8" is attaching, can't finish within the alloted time
#+END_SRC

When you then do an ~openstack volume list~, you will see something like this where a volume is stuck in "reserved":

#+BEGIN_SRC shell
|--------------------------------------+------------------------------------------+----------|
| ID                                   | Name                                     | Status   |
|--------------------------------------+------------------------------------------+----------|
| 25c25c5d-75cb-48fd-a9c4-4fd680bea79b | pvc-41d76080-6ad7-11ea-a62a-fa163ebb95dd | reserved |
|--------------------------------------+------------------------------------------+----------|
#+END_SRC

You (or if you do not have permission, Jetstream staff) can reset the volume with:

#+BEGIN_SRC shell
  openstack volume set --state available <volume uuid>
#+END_SRC

or with

#+BEGIN_SRC shell
      openstack volume list | grep -i reserved | awk \
          'BEGIN { FS = "|" } ; { print $2 }' | xargs -n1 openstack volume set \
      --state available
#+END_SRC

The problem is that once a volume gets stuck like this, it tends to happen again and again. In this scenario, [[#h-CB601D7B][you have to provide a long term solution to the user]].

**** Script to Mitigate Problem
     :PROPERTIES:
     :CUSTOM_ID: h-F7B1FC52
     :END:

Invoking this script (e.g., call it ~notify.sh~) from crontab, maybe every three minutes or so, can help mitigate the problem and give you faster notification of the issue. Note [[https://ifttt.com][iftt]] is a push notification service with webhooks available that can notify your smart phone triggered by a ~curl~ invocation as demonstrated below. You'll have to create an ifttt login and download the app on your smart phone.

#+BEGIN_SRC shell
  #!/bin/bash

  source /home/rocky/.bash_profile

  VAR=$(openstack volume list -f value -c ID -c Status | grep -i reserved | wc -l)

  MSG="Subject: Volume Stuck in Reserved on Jetstream"

  if [[ $VAR -gt 0 ]]
  then
      echo $MSG | /usr/sbin/sendmail my@email.com
      openstack volume list | grep -i reserved >> /tmp/stuck.txt
      curl -X POST https://maker.ifttt.com/trigger/jetstream/with/key/xyz
      openstack volume list -f value -c ID -c Status | grep -i reserved | awk \
          '{ print $1 }' | xargs -n1 openstack volume set --state available
  fi
#+END_SRC

you can invoke this script from crontab:

#+BEGIN_SRC shell
  */3 * * * * /home/rocky/notify.bash > /dev/null 2>&1
#+END_SRC

Note, again, this is just a temporary solution. You still have to provide a longer-term workaround described in the next section:

**** Not a Solution but a Longer Term Workaround
     :PROPERTIES:
     :CUSTOM_ID: h-CB601D7B
     :END:

[[#h-1765D7EB][With the volume ID obtained earlier]], issue:

#+BEGIN_SRC shell
  openstack volume attachment list --os-volume-api-version 3.27 | grep -i d910c7fae38b
#+END_SRC

which will yield something like:

#+BEGIN_SRC shell
  | 67dbf5c3-c190-4f9e-a2c9-78da44df6c75 | cf1a7adf-7b0a-422f-8843-d910c7fae38b | reserved  | 0593faaf-8ba0-4eb5-84ad-b7282ce5aac2 |
#+END_SRC

At this point, you may see /two/ entries (even though only one is shown here). One attachment in reserved and one that is attached.

Next, delete the reserved attachment:

#+BEGIN_SRC shell
  cinder attachment-delete 67dbf5c3-c190-4f9e-a2c9-78da44df6c75
#+END_SRC

*** Renew Expired K8s Certificates
:PROPERTIES:
:CUSTOM_ID: h-60D08FB6
:END:

**** Background
:PROPERTIES:
:CUSTOM_ID: h-01F8D10F
:END:

Kubernetes clusters use PKI certificates to allow the different components of K8s to communicate and authenticate with one another. See the [[https://kubernetes.io/docs/setup/best-practices/certificates/][official docs]] for more information. When firing up a JupyterHub cluster using the procedures outlined in this documentation, the certificates are automatically generated for us on cluster creation, however they expire after a full year. You can check the expiration date of your current certificates by running the following on the master node of the cluster:

#+BEGIN_SRC shell
  sudo kubeadm alpha certs check-expiration
#+END_SRC

Once the certificates have expired, you will be unable to run, for example, ~kubectl~ commands, and the [[https://kubernetes.io/docs/setup/best-practices/certificates/][control plane components]] will not be able to, for example, fire up new pods, ie new JupyterLab servers, nor perform ~helm~ upgrades to the server. Example output of running ~kubectl~ commands with expired
certificates is:

#+BEGIN_SRC shell
  # kubectl get pods -n jhub
  Unable to connect to the server: x509: certificate has expired or is not yet valid: current time 2022-06-29T23:09:31Z is after 2022-06-28T17:38:37Z
#+END_SRC

**** Resolution
:PROPERTIES:
:CUSTOM_ID: h-0A5DF245
:END:

There are a number of ways to renew certificates outlined in the [[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/][official docs]]. Here, the manual renewal method is outlined. While this procedure should be non-destructive, it is recommended to have users backup data/notebooks before this is done. In addition, one of the steps requires a manual restart of the control plane pods, which means the Hub (and potentially user servers) may suffer a small amount of downtime.

All commands are ran on the master node of the cluster. In addition, the documentation does not include the ~alpha~ portion of the ~kubeadm~ commands outlined below. This is required: see the answer to [[https://serverfault.com/questions/1051333/how-to-renew-a-certificate-in-kubernetes-1-12][this]] question.

First, confirm that your certificates truly are expired:

#+BEGIN_SRC shell
  sudo kubeadm alpha certs check-expiration
#+END_SRC

Then, run the renewal command to renew all certs:

#+BEGIN_SRC shell
   sudo kubeadm alpha certs renew all
#+END_SRC

Double check the certificates were renewed:

#+BEGIN_SRC shell
  sudo kubeadm alpha certs check-expiration
#+END_SRC

Now, we must restart the control plane pods. We do this by moving the files found in ~/etc/kubernetes/manifests~ to a temporary place, waiting for the [[https://serverfault.com/questions/1051333/how-to-renew-a-certificate-in-kubernetes-1-12][kubelet]] to recognize the change in the manifests, and tear down the pods. Once this is done, the files can be moved back into ~/etc/kubernetes/manifests~, and we can wait for the kubelet to respawn the pods. Finally, reset the ~~/.kube/config~ file and run ~kubectl~ commands.

#+BEGIN_SRC shell
  ###
  # All commands ran on the master node
  ###

  # Copy manifests
  mkdir ~/manifestsBackup_yyyy_mm_dd
  sudo cp /etc/kubernetes/manifests/* ~/manifestsBackup_yyyy_mm_dd/

  # Sanity check
  ls ~/manifestsBackup_yyyy_mm_dd

  # Navigate to /etc/kubernetes/manifests and list files, to ensure we're removing
  # what we think we are
  cd /etc/kubernetes/manifests
  ls

  # Verify the containers you are about to remove are currently running
  sudo docker ps

  # Remove files
  rm ./*

  # Wait until the containers are removed
  sudo docker ps

  # Replace files
  sudo cp ~/manifestsBackup_yyyy_mm_dd/* /etc/kubernetes/manifests/

  # Wait until containers are respawned
  sudo docker ps

  # Reset the config
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

  # Cross your fingers and hope you can now run kubectl commands again!
  kubectl get pods --all-namespaces
#+END_SRC

If you want to run kubectl commands from another machine, for example the machine where we launch JupyterHubs from within docker containers, you must copy this config file to that machine's ~$HOME/.kube~ directory.

You should have the IP and ~ssh~ access of/to the master node. Copy over the config through ~scp~:

#+BEGIN_SRC shell
  ###
  # On the appropriate "Jupyter control center" docker container
  ###

  # Directory probably already exists, but try creating the directory anyways
  mkdir $HOME/.kube
  scp ubuntu@<ip>:~/.kube/config $HOME/.kube/config
#+END_SRC

Finally, edit the ~server~ value in the =$HOME/.kube/config= to point to ~127.0.0.1~, as kubectl will communicate with the api-server through a tunnel created on the Jupyter control container. See [[../../../openstack/bin/kube-setup2.sh][this]] script and the reference
therein for the reason behind doing this.

#+BEGIN_SRC shell
  # Change a line that looks like the following
  server: https://<some-ip>:6443
  # to
  server: https://127.0.0.1:6443
#+END_SRC

You should now be able to run ~kubectl~ commands, fire up new user servers, and run ~helm~ upgrades.

*** Evicted Pods Due to Node Pressure
:PROPERTIES:
:CUSTOM_ID: h-CEF2540C
:END:

If a node starts to run out of resources and you try to fire up new pods on it, the pods will have the "evicted" status associated with them. This can happen when trying to update a JupyterHub whose single user JupyterLab images are large, as Jetstream2's ~m3.medium~ instances only have ~60GB~ of disk storage.

This problem was first noticed when updating MVU's JupyterHub, whose single user image was on the order of ~10GB~. The new JupyterLab image was going to be similarly large.

Unless otherwise stated, all output of shell commands are from the ~mvu-test~ cluster.

This is what a "healthy" cluster looks like:

#+begin_src shell
  $ kubectl get pods -n jhub
  NAME                              READY   STATUS    RESTARTS   AGE
  continuous-image-puller-kc4v5     1/1     Running   0          21h
  hub-64747d5848-x6z7s              1/1     Running   0          21h
  proxy-6675c69dd4-47b4d            1/1     Running   0          10d
  user-scheduler-79c85f98dd-r7gl4   1/1     Running   0          10d
  user-scheduler-79c85f98dd-vqz24   1/1     Running   0          10d

  $ kubectl get nodes -n jhub
  NAME                     STATUS   ROLES                  AGE   VERSION
  mvu-test-1               Ready    control-plane,master   10d   v1.22.5
  mvu-test-k8s-node-nf-1   Ready    <none>                 10d   v1.22.5
#+END_SRC

If we inspect the worker node, we will see the following relevant
information:

#+BEGIN_SRC shell
  $ kubectl describe node -n jhub mvu-test-k8s-node-nf-1 | less
  Conditions:
    Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
    ----                 ------  -----------------                 ------------------                ------                       -------
    NetworkUnavailable   False   Mon, 06 Feb 2023 03:39:57 +0000   Mon, 06 Feb 2023 03:39:57 +0000   FlannelIsUp                  Flannel is running on this node
    MemoryPressure       False   Thu, 16 Feb 2023 23:39:42 +0000   Mon, 06 Feb 2023 03:39:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
    DiskPressure         False   Thu, 16 Feb 2023 23:39:42 +0000   Thu, 16 Feb 2023 01:43:51 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
    PIDPressure          False   Thu, 16 Feb 2023 23:39:42 +0000   Mon, 06 Feb 2023 03:39:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
    Ready                True    Thu, 16 Feb 2023 23:39:42 +0000   Mon, 06 Feb 2023 03:39:59 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
#+END_SRC

The "Conditions" field describes that the node is undergoing no [[https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/][Node Pressure]]. If the node were experiencing some kind of node pressure, attempting to create any pods would cause them to become stuck in the "evicted" state. By [[https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#hard-eviction-thresholds][default]], pods will be evicted from a node if the available storage space on the node falls below ~10%~.

Attempting to re-deploy the JupyterHub with a new image will cause the JupyterHub to pull in the new image. If this results in disk pressure, you will see Kubernetes create pods that receive the "Evicted" state:

#+BEGIN_SRC shell
  $ bash install_jhub.sh

  # In a seperate shell
  $ kubectl get pods -n jhub
  NAME                              READY   STATUS    RESTARTS   AGE
  continuous-image-puller-hm25g     0/1     Evicted   0          36s
  hook-image-awaiter--1-4zmsc       0/1     Pending   0          43s
  hook-image-puller-f4ffs           0/1     Evicted   0          12s
  hub-85c77d5fd-9zhb5               0/1     Pending   0          98s
  jupyter-robertej09                1/1     Running   0          19m
  proxy-6675c69dd4-47b4d            1/1     Running   0          10d
  user-scheduler-79c85f98dd-r7gl4   1/1     Running   0          10d
  user-scheduler-79c85f98dd-vqz24   1/1     Running   0          10d
#+END_SRC

The node will show that it is indeed experiencing disk pressure:

#+BEGIN_SRC shell
  $ kubectl describe node -n jhub <node-name> | less # and scroll down
  Conditions:
    Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
    ----                 ------  -----------------                 ------------------                ------                       -------
    NetworkUnavailable   False   Mon, 06 Feb 2023 03:39:57 +0000   Mon, 06 Feb 2023 03:39:57 +0000   FlannelIsUp                  Flannel is running on this node
    MemoryPressure       False   Fri, 17 Feb 2023 00:48:47 +0000   Mon, 06 Feb 2023 03:39:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
    DiskPressure         True    Fri, 17 Feb 2023 00:48:47 +0000   Fri, 17 Feb 2023 00:45:37 +0000   KubeletHasDiskPressure       kubelet has disk pressure
    PIDPressure          False   Fri, 17 Feb 2023 00:48:47 +0000   Mon, 06 Feb 2023 03:39:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
    Ready                True    Fri, 17 Feb 2023 00:48:47 +0000   Mon, 06 Feb 2023 03:39:59 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
#+END_SRC

Scrolling to the "Events" section of the ~describe node~ output, you may find that Kubernetes is attempting to salvage the install by freeing up storage space. This particular output was created while JupyterHub was being upgraded on the "main" ~mvu-23s~ cluster:

#+BEGIN_SRC shell
  Events:
    Type     Reason                 Age                From     Message
    ----     ------                 ----               ----     -------
    Warning  EvictionThresholdMet   55m (x4 over 17d)  kubelet  Attempting to reclaim ephemeral-storage
    Normal   NodeHasDiskPressure    55m (x4 over 17d)  kubelet  Node mvu23s-k8s-node-nf-9 status is now: NodeHasDiskPressure
    Normal   NodeHasNoDiskPressure  50m (x5 over 29d)  kubelet  Node mvu23s-k8s-node-nf-9 status is now: NodeHasNoDiskPressure
#+END_SRC

In this case, Kubernetes successfully performed garbage collection and was able to recover enough storage space to complete the install after some period of waiting.

If Kubernetes is taking too long to want to perform garbage collection, there is a very hacky work-around to this. Cancel the installation (~ctrl-c~), and run ~helm uninstall jhub -n jhub~. This will uninstall the JupyterHub from the cluster, however, importantly it will [[https://www.zonca.dev/posts/2018-09-24-jetstream_kubernetes_kubespray_jupyterhub#delete-and-reinstall-jupyterhub][keep user data intact]].

Through some inspection, you may find that the worker nodes contain a cache of not only the single user image which is currently deployed, but the previous one as well:

#+BEGIN_SRC shell
  $ kubectl get nodes -o yaml | less # after scrolling down you'll eventually see in the worker node
      images:
      - names:
        - docker.io/unidata/mvu-spring-2023@sha256:a092260d963474b04b71f9b2887faaa879ed0e61d3b2867972308e962b41d7dc
        - docker.io/unidata/mvu-spring-2023:2023Feb16_001123_74af5561
        sizeBytes: 2656565418
      - names:
        - docker.io/unidata/mvu-spring-2023@sha256:2a257f0673482a110dd73b42f91854ecc2d7a3244aa7fd34c988b2fb591d4335
        - docker.io/unidata/mvu-spring-2023:2023Feb04_021143_912787ce
        sizeBytes: 2653100806
#+END_SRC

The work-around is to force the removal of one of the images by installing a small single user image in the JupyterHub that you know will fit on the node's available storage space. The [[https://hub.docker.com/r/jupyter/base-notebook/tags][jupyter/base-notebook]] image is a good candidate for this. Edit the appropriate sections of ~secrets.yaml~ to install this smaller image, run ~bash install_jhub.sh~, and watch ~kubectl get pods -n jhub~ to ensure everything installs correctly. Kubernetes should have purged one of the previous images and freed up storage space. Now, re-edit ~secrets.yaml~ and install the image you desire.
*** Updating Openstack Credentials for Kubernetes
:PROPERTIES:
:CUSTOM_ID: h-FABFCED0
:END:

If your openstack credentials expire, you will be unable to run even basic ~openstack~ commands such as ~openstack server list~. Generally, this would not be a pressing issue, as instances that are already running should stay running. However, expired openstack credentials pose a large problem for JupyterHubs that have been deployed using Kubernetes, as K8s uses openstack credentials to communicate with the Jetstream2 cloud and perform essential functions such as mounting openstack volumes on pods. This results in the user receiving a message such as the following when attempting to spawn their single user server:

#+begin_example
  Your server is starting up.
  You will be redirected automatically when it's ready for you.
  72% Complete
  2023-04-14T16:39:44Z [Warning] Unable to attach or mount volumes: unmounted volumes=[volume-<user>], unattached volumes=[volume-<user>]: timed out waiting for the condition
  Event log
#+end_example

Doing a ~kubectl describe pod -n jhub <single-user-pod>~ on the offending pod will reveal that the volume is failing to attach due to an authentication issue.

**** Creating New Credentials
:PROPERTIES:
:CUSTOM_ID: h-6F9D771F
:END:

Follow the instructions in the [[https://docs.jetstream-cloud.org/ui/cli/auth/#using-the-horizon-dashboard-to-generate-openrcsh][Jetstream2 docs]] to navigate to the "Application Credentials" page of the Horizon interface. From here, you can verify that your credentials are expired and create a new set of credentials.

Once you've created the new credentials, you can update any necessary ~openrc.sh~ files. Note that it is [[https://medium.com/@jonsbun/why-need-to-be-careful-when-mounting-single-files-into-a-docker-container-4f929340834][important]] to use a text editor, such as ~nano~, that will not change the inode of the file being edited, as docker mounts files by their inode and not their file name. Once this has been done, you will have to re-source ~openrc.sh~ for the changes to take effect: ~source /path/to/openrc.sh~. Ensure you are able to use these new credentials to run openstack commands: ~openstack server list~.

**** Updating Credentials in K8s
:PROPERTIES:
:CUSTOM_ID: h-4126F02C
:END:

Start a shell with ~kubectl~ capabilities for the cluster. Follow Andrea Zonca's [[https://www.zonca.dev/posts/2023-03-23-update-openstack-credentials-kubernetes][instructions]] to update the credentials. The procedure is outlined below, and has to be repeated for two Kubernetes [[https://kubernetes.io/docs/concepts/configuration/secret/][Secrets]], ~external-openstack-cloud-config~ and ~cloud-config~.

#+begin_example
  # Print out the base64 encoded secret
  kubectl get secret -n kube-system <secret-name> -o jsonpath='{.data}'

  # Copy/paste the secret to decode it; dump to file
  echo <secret> | base64 --decode > /tmp/cloud.conf

  # Update the temporary with the new credentials
  vim /tmp/cloud.conf

  # Re-encode the secret; copy the terminal output
  cat /tmp/cloud.conf | base64 -w0

  # Edit update the K8s secret
  kubectl edit secret -n kube-system <secret-name>
#+end_example

Once both secrets have been updated, restart the cluster via openstack for changes to take effect

#+begin_example
  # Ensure you're rebooting what you think you are
  for INSTANCE in $(openstack server list -c Name -f value | grep <PATTERN>); do echo "openstack server reboot $INSTANCE"; done
  # Reboot
  for INSTANCE in $(openstack server list -c Name -f value | grep <PATTERN>); do openstack server reboot $INSTANCE; done
#+end_example
