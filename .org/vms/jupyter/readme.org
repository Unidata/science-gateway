#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: auto-id:t
#+TITLE: Creating  a JupyterHub on Jetstream with the Zero to JupyterHub Project
#+DATE:  <2017-06-26 Mon>
#+AUTHOR: Julien Chastang
#+EMAIL: chastang@ucar.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.2 (Org mode 9.0.5)

* Creating  a JupyterHub on Jetstream with the Zero to JupyterHub Project
  :PROPERTIES:
  :CUSTOM_ID: h-D73CBC56
  :END:

** Kubernetes Cluster
   :PROPERTIES:
   :CUSTOM_ID: h-65F9358E
   :END:

*** jupyterhub.sh
    :PROPERTIES:
    :CUSTOM_ID: h-B56E19AB
    :END:

~jupyterhub.sh~ and the related ~z2j.sh~ are convenience scripts similar to ~openstack.sh~ to give you access to a pre-configured environment that will allow you to build and/or run a Zero to JupyterHub cluster. It also relies on the [[file:../../openstack/readme.org::#h-4A9632CC][same Docker container]] as the ~openstack.sh~ script. ~jupyterhub.sh~ takes one argument with the ~-n~ option, the name of the Zero to JupyterHub cluster.  Invoke it from the =science-gateway/openstack= directory. ~jupyterhub.sh~ and the related ~z2j.sh~ ensure the information for this Zero to JupyterHub cluster is persisted outside the container via Docker file mounts -- otherwise all the information about this cluster  would be confined in memory inside the Docker container. The vital information will be persisted in a local =jhub= directory.

*** Create Cluster
    :PROPERTIES:
    :CUSTOM_ID: h-2FF65549
    :END:

[[file:../../openstack/readme.org::#h-DA34BC11][Create a Kubernetes cluster]] with the desired number of nodes and VM sizes. Lock down the master node of the cluster per Unidata security procedures. Work with sys admin staff to obtain a DNS name (e.g., jupyterhub.unidata.ucar.edu), and a certificate from a certificate authority for the master node.

** unidata/unidatahub Docker Container
   :PROPERTIES:
   :CUSTOM_ID: h-CD007D2A
   :END:

Build the Docker container in this directory and push it to dockerhub.

#+BEGIN_SRC sh
  docker build -t unidata/unidatahub:`openssl rand -hex 6` . > /tmp/docker.out 2>&1 &
  docker push unidata/unidatahub:<container id>
#+END_SRC

** Configure and Deploy the JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-E5CA5D99
   :END:

From the client host where you created the Kubernetes cluster, follow [[https://zonca.dev/2020/06/kubernetes-jetstream-kubespray.html#install-jupyterhub][Andrea Zonca's instructions]].

After you have created the ~secrets.yaml~ as instructed, customize it with the choices below

*** Letsencrypt versus Certificate from a Certificate Authority
    :PROPERTIES:
    :CUSTOM_ID: h-294A4A20
    :END:

**** Letsencrypt
     :PROPERTIES:
     :CUSTOM_ID: h-E1082806
     :END:

Follow [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's instructions]] on setting up letsencrypt along with this yaml snippet below. Replace the hostname where appropriate.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: certmanager-tls-jupyterhub
#+END_SRC

**** Certificate from CA
     :PROPERTIES:
     :CUSTOM_ID: h-205AEDAB
     :END:

Work with sys admin staff to obtain a certificate from a CA.

Follow [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's instructions]] on setting up HTTPS with custom certificates. Note that when adding the key with

#+BEGIN_SRC shell
 kubectl create secret tls <cert-secret> --key ssl.key --cert ssl.crt -n jhub
#+END_SRC

supply the base and intermediate certificates and not the full chain certificate (i.e., with root certificates).

Here is a snippet of what the ingress configuration will look like in the ~secrets.yaml~.

#+BEGIN_SRC yaml
  ingress:
    enabled: true
    annotations:
      kubernetes.io/tls-acme: "true"
    hosts:
      - <jupyterhub-host>
    tls:
        - hosts:
           - <jupyterhub-host>
          secretName: <secret_name>
#+END_SRC

*** OAuth Authentication
  :PROPERTIES:
  :CUSTOM_ID: h-8A3C5434
  :END:

**** Globus
  :PROPERTIES:
  :CUSTOM_ID: h-C0E8193F
  :END:

[[https://developers.globus.org/][Globus OAuth capability]] is available for user authentication. The instructions [[https://github.com/jupyterhub/oauthenticator#globus-setup][here]] are relatively straightforward.

#+BEGIN_SRC yaml
  auth:
    type: globus
    globus:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
      identityProvider: "xsede.org"
    admin:
      users:
        - adminuser1
#+END_SRC

**** GitHub
     :PROPERTIES:
     :CUSTOM_ID: h-BB3C66CD
     :END:

Setup an OAuth app on GitHub

#+BEGIN_SRC yaml
  auth:
    type: github
    github:
      clientId: "xxx"
      clientSecret: "xxx"
      callbackUrl: "https://<jupyterhub-host>:443/oauth_callback"
    admin:
      users:
        - adminuser1
#+END_SRC
*** unidata/unidatahub
    :PROPERTIES:
    :CUSTOM_ID: h-214D1D4C
    :END:

Add the Unidata JupyterHub configuration (~unidata/unidatahub~) and related items (e.g., pulling of Unidata Python projects). Customize the desired CPU / RAM usage. [[https://docs.google.com/spreadsheets/d/15qngBz4L5gwv_JX9HlHsD4iT25Odam09qG3JzNNbdl8/edit?usp=sharing][This spreadsheet]] will help you determine the size of the cluster based on number of users, desired cpu/user, desired RAM/user. Duplicate it and adjust it for your purposes.

#+INCLUDE: "../../../vms/jupyter/secrets.yaml" src yaml :lines "35-60"

** Navigate to JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-209E2FBC
   :END:

In a web browser, navigate to your newly minted JupyterHub and see if it is as you expect.

** Tearing Down JupyterHub
   :PROPERTIES:
   :CUSTOM_ID: h-1E027567
   :END:

*** Total Destructive Tear Down
    :PROPERTIES:
    :CUSTOM_ID: h-A69ADD92
    :END:

Tearing down the JupyterHub including user OpenStack volumes is possible. From the Helm and Kubernetes client:

#+BEGIN_SRC sh
  helm uninstall jhub -n jhub
  # Think before you type !
  echo $CLUSTER; sleep 60; kubectl delete namespace jhub
#+END_SRC

To further tear down the Kubernetes cluster see [[file:../../openstack/readme.org::#h-DABDACC7][Tearing Down the Cluster]].

*** Tear Down While Preserving User Volumes
    :PROPERTIES:
    :CUSTOM_ID: h-5F2AA05F
    :END:

A gentler tear down that preserves the user volumes is described in [[https://zonca.github.io/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html][Andrea's documentation]]. See the section on "persistence of user data".
** Troubleshooting
   :PROPERTIES:
   :CUSTOM_ID: h-0E48EFE9
   :END:
*** Unresponsive JupyterHub
    :PROPERTIES:
    :CUSTOM_ID: h-FF4348F8
    :END:

**** Preliminary Work
     :PROPERTIES:
     :CUSTOM_ID: h-C2429D6E
     :END:

If a JupyterHub becomes unresponsive (e.g., 504 Gateway Time-out), login in to the Kubernetes client and do preliminary backup work in case things go badly. First:

#+BEGIN_SRC shell
  kubectl get pvc -n jhub -o yaml > pvc.yaml.ro
  kubectl get pv -n jhub -o yaml > pv.yaml.ro
  chmod 400 pvc.yaml.ro pv.yaml.ro
#+END_SRC

Make ~pvc.yaml.ro~ ~pv.yaml.ro~ read only since these files could become precious in  case you have to do data recovery for users. More on this subject below.

**** Delete jhub Pods
     :PROPERTIES:
     :CUSTOM_ID: h-6404011E
     :END:

Next, start investigating by issuing:

#+BEGIN_SRC shell
  kubectl get pods -n jhub
#+END_SRC

this command will yield something like

#+BEGIN_SRC shell
  NAME                      READY   STATUS    RESTARTS   AGE
  hub-5bdccd4784-lzw87      1/1     Running   0          17h
  jupyter-joe               1/1     Running   0          4h51m
  proxy-7b986cdb75-mhl86    1/1     Running   0          29d
#+END_SRC

Now start deleting the ~jhub~ pods starting with the user pods (e.g., ~jupyter-joe~).

#+BEGIN_SRC
  kubectl delete pod <pod name> -n jhub
#+END_SRC

Check to see if the JupyterHub is reachable. If it is not, keep deleting pods checking for reachability after each pod deletion.

**** Delete jhub, But Do Not Purge Namespace
     :PROPERTIES:
     :CUSTOM_ID: h-1C4D98E6
     :END:

If the JupyterHub is still not reachable, you can try deleting and recreating the JupyterHub but *do not* delete the namespace as you will wipe out user data.

#+BEGIN_SRC shell
  helm uninstall jhub -n jhub
  # But DO NOT issue this command
  # kubectl delete namespace jhub
#+END_SRC

Then try reinstalling with

#+BEGIN_SRC
  bash install_jhub.sh
#+END_SRC

Now, try recover user volumes as [[https://zonca.dev/2018/09/kubernetes-jetstream-kubespray-jupyterhub.html#delete-and-recreate-openstack-instances][described at the end of the section here]] with the ~pvc.yaml.ro~ ~pv.yaml.ro~ saved earlier (make writable copies of those ~ro~ files). If that still does not work, you can try destroying the entire cluster and recreating it as described in that same link.

*** Volumes Stuck in Reserved State
    :PROPERTIES:
    :CUSTOM_ID: h-354DE174
    :END:

**** Background
     :PROPERTIES:
     :CUSTOM_ID: h-1765D7EB
     :END:

Occasionally, when logging into a JupyterHub the user will encounter a volume attachment error that causes a failure in the login process. [[https://github.com/zonca/jupyterhub-deploy-kubernetes-jetstream/issues/40][This is an ongoing issue on Jetstream that we have never been able to get to the bottom of]]. The user will see an error that looks something like:

#+BEGIN_SRC shell
2020-03-27 17:54:51+00:00 [Warning] AttachVolume.Attach failed for volume "pvc-5ce953e4-6ad9-11ea-a62a-fa163ebb95dd" : Volume "0349603a-967b-44e2-98d1-0ba1d42c37d8" is attaching, can't finish within the alloted time
#+END_SRC

When you then do an ~openstack volume list~, you will see something like this where a volume is stuck in "reserved":

#+BEGIN_SRC shell
|--------------------------------------+------------------------------------------+----------|
| ID                                   | Name                                     | Status   |
|--------------------------------------+------------------------------------------+----------|
| 25c25c5d-75cb-48fd-a9c4-4fd680bea79b | pvc-41d76080-6ad7-11ea-a62a-fa163ebb95dd | reserved |
|--------------------------------------+------------------------------------------+----------|
#+END_SRC

You (or if you do not have permission, Jetstream staff) can reset the volume with:

#+BEGIN_SRC shell
  openstack volume set --state available <volume uuid>
#+END_SRC

or with

#+BEGIN_SRC shell
      openstack volume list | grep -i reserved | awk \
          'BEGIN { FS = "|" } ; { print $2 }' | xargs -n1 openstack volume set \
      --state available
#+END_SRC

The problem is that once a volume gets stuck like this, it tends to happen again and again. In this scenario, [[#h-CB601D7B][you have to provide a long term solution to the user]].

**** Script to Mitigate Problem
     :PROPERTIES:
     :CUSTOM_ID: h-F7B1FC52
     :END:

Invoking this script (e.g., call it ~notify.sh~) from crontab, maybe every three minutes or so, can help mitigate the problem and give you faster notification of the issue. Note [[https://ifttt.com][iftt]] is a push notification service with webhooks available that can notify your smart phone triggered by a ~curl~ invocation as demonstrated below. You'll have to create an ifttt login and download the app on your smart phone.

#+BEGIN_SRC shell
  #!/bin/bash

  source /home/centos/.bash_profile

  VAR=$(openstack volume list -f value -c ID -c Status | grep -i reserved | wc -l)

  MSG="Subject: Volume Stuck in Reserved on Jetstream"

  if [[ $VAR -gt 0 ]]
  then
      echo $MSG | /usr/sbin/sendmail my@email.com
      openstack volume list | grep -i reserved >> /tmp/stuck.txt
      curl -X POST https://maker.ifttt.com/trigger/jetstream/with/key/xyz
      openstack volume list -f value -c ID -c Status | grep -i reserved | awk \
          '{ print $1 }' | xargs -n1 openstack volume set --state available
  fi
#+END_SRC

you can invoke this script from crontab:

#+BEGIN_SRC shell
  */3 * * * * /home/centos/notify.bash > /dev/null 2>&1
#+END_SRC

Note, again, this is just a temporary solution. You still have to provide a longer-term workaround described in the next section:

**** Not a Solution but a Longer Term Workaround
     :PROPERTIES:
     :CUSTOM_ID: h-CB601D7B
     :END:

[[#h-1765D7EB][With the volume ID obtained earlier]], issue:

#+BEGIN_SRC shell
  cinder attachment-list  | grep -i d910c7fae38b
  #+END_SRC

which will yield something like:

#+BEGIN_SRC shell
  WARNING:cinderclient.shell:API version 3.62 requested,
  WARNING:cinderclient.shell:downgrading to 3.55 based on server support.
  | 67dbf5c3-c190-4f9e-a2c9-78da44df6c75 | cf1a7adf-7b0a-422f-8843-d910c7fae38b | reserved  | 0593faaf-8ba0-4eb5-84ad-b7282ce5aac2 |
  #+END_SRC

At this point, you may see /two/ entries (even though only one is shown here). One attachment in reserved and one that is attached.

Next, delete the reserved attachment:

#+BEGIN_SRC shell
  cinder attachment-delete 67dbf5c3-c190-4f9e-a2c9-78da44df6c75
  #+END_SRC
