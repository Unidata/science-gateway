#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline author:t
#+OPTIONS: broken-links:nil c:nil creator:nil d:(not "LOGBOOK") date:t e:t
#+OPTIONS: email:nil f:t inline:t num:t p:nil pri:nil prop:nil stat:t tags:t
#+OPTIONS: tasks:t tex:t timestamp:t title:t toc:t todo:t |:t
#+OPTIONS: auto-id:t
#+TITLE: Creating an IDD Archiver VM on Jetstream
#+DATE:  <2017-04-26 Wed>
#+AUTHOR: Julien Chastang
#+EMAIL: chastang@ucar.edu
#+LANGUAGE: en
#+SELECT_TAGS: export
#+EXCLUDE_TAGS: noexport
#+CREATOR: Emacs 25.1.2 (Org mode 9.0.5)

#+PROPERTY: header-args :noweb yes :eval no

* Creating an IDD Archiver VM on Jetstream
  :PROPERTIES:
  :CUSTOM_ID: h-046F9FE1
  :END:
** Create an IDD Archiver VM on Jetstream
   :PROPERTIES:
   :CUSTOM_ID: h-304AA966
   :END:

Create an ~m1.medium~ VM with the [[file:../../openstack/readme.org::#h-03303143][Jetstream OpenStack API]]. [[file:../../openstack/readme.org::#h-9BEEAB97][Create and attach]] a 5TB =/data= volume to that VM. Work with Unidata system administrator staff to have this VM's IP address resolve to =idd-archiver.scigw.unidata.ucar.edu=.

** Clone the science-gateway and TdsConfig Repositories
   :PROPERTIES:
   :CUSTOM_ID: h-00BE67D7
   :END:

We will be making heavy use of the ~Unidata/science-gateway~ git repository.

#+BEGIN_SRC shell :tangle no :exports code
  git clone https://github.com/Unidata/science-gateway ~/science-gateway
#+END_SRC

In addition, we will employ the ~Unidata/TdsConfig~ repository to obtain our LDM pqacts. We will *not* be running the TDS on the IDD archiver VM.

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh :exports code :shebang "#!/bin/bash"
  git clone https://github.com/Unidata/TdsConfig ~/TdsConfig
#+END_SRC

** Prepare IDD Archiver for Docker and docker-compose
   :PROPERTIES:
   :CUSTOM_ID: h-FF66923F
   :END:

With the help of Docker and ~docker-compose~, starting a VM containing an IDD archiver is relatively simple. [[file:../../vm-init-readme.org::#h-786799C4][See here to install Docker and docker-compose]].

** ~/etc Directory
   :PROPERTIES:
   :CUSTOM_ID: h-B5A9CA86
   :END:

This =~/etc= directory will contain your LDM configuration files.

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  mkdir -p ~/etc
  cp ~/science-gateway/vms/idd-archiver/etc/* ~/etc/
#+END_SRC

*** ~/etc/ldmd.conf
    :PROPERTIES:
    :CUSTOM_ID: h-A598B286
    :END:

You may have to tailor the =ldmd.conf= to your data feed requirements. In addition, change the following request line

#+BEGIN_SRC shell :tangle no
  REQUEST ANY ".*" 10.0.0.21
#+END_SRC

to point the local IDD relay *10.0 address*.

or something like

#+BEGIN_SRC shell :tangle no
  REQUEST CONDUIT ".*" 10.0.0.21
  REQUEST NGRID ".*" 10.0.0.21
  REQUEST NOTHER ".*" 10.0.0.21
  REQUEST NEXRAD3 ".*" 10.0.0.21
  REQUEST ANY-NEXRAD3-NOTHER-NGRID-CONDUIT ".*" 10.0.0.21
#+END_SRC

to break apart the requests.

*** ~/etc/registry.xml
    :PROPERTIES:
    :CUSTOM_ID: h-27A09559
    :END:

Verify the =registry.xml= file is updated the ~hostname~ element with ~idd-archiver.jetstream-cloud.org~ so that Real-Time IDD statistics can be properly reported back to Unidata. Finally, you may have to adjust the size of the queue currently at ~6G~.

** Data Scouring
   :PROPERTIES:
   :CUSTOM_ID: h-1CA59DB7
   :END:
Scouring the =/data/ldm= directory is achieved through the LDM =scour.conf= mechanism and scouring utilities. See the [[https://github.com/Unidata/ldm-docker][ldm-docker project README]] for details. Examine the =etc/scour.conf=, =cron/ldm=, and =docker-compose.yml= to ensure scouring of data happens in the time frame you wish.
** pqacts
   :PROPERTIES:
   :CUSTOM_ID: h-4BDFE35D
   :END:

Unpack the pqacts configurations from the ~TdsConfig~ project and put them in the expected =~/etc/TDS= location.

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  mkdir -p ~/tdsconfig/ ~/etc/TDS
  wget http://unidata-tds.s3.amazonaws.com/tdsConfig/thredds/config.zip -O ~/tdsconfig/config.zip
  unzip ~/tdsconfig/config.zip -d ~/tdsconfig/
  cp -r ~/tdsconfig/pqacts/* ~/etc/TDS
#+END_SRC

** Edit ldmfile.sh
   :PROPERTIES:
   :CUSTOM_ID: h-D2BD1E3A
   :END:

Examine the =~/etc/TDS/util/ldmfile.sh= file. As the top of this file indicates, you must change the =logfile= to suit your needs. Change the

#+BEGIN_EXAMPLE
  logfile=logs/ldm-mcidas.log
#+END_EXAMPLE

line to

#+BEGIN_EXAMPLE
  logfile=var/logs/ldm-mcidas.log
#+END_EXAMPLE

This will ensure =ldmfile.sh= can properly invoked from the =pqact= files.

We can achieve this change with a bit of ~sed~:

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  # in place change of logs dir w/ sed
  sed -i s/logs\\/ldm-mcidas.log/var\\/logs\\/ldm-mcidas\\.log/g \
      ~/etc/TDS/util/ldmfile.sh
#+END_SRC

Also ensure that =ldmfile.sh= is executable.

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  chmod +x ~/etc/TDS/util/ldmfile.sh
#+END_SRC

** /data/ldm/queues Directory
   :PROPERTIES:
   :CUSTOM_ID: h-2428D469
   :END:

This =queues= directory will contain the LDM queue file.

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  mkdir -p /data/ldm/queues
#+END_SRC

** /data/ldm/logs Directory
   :PROPERTIES:
   :CUSTOM_ID: h-57DC40FF
   :END:

Create the LDM =logs= directory.

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  mkdir -p /data/ldm/logs/
#+END_SRC

** Ensure /data Volume Availability Upon Machine Restart
   :PROPERTIES:
   :CUSTOM_ID: h-3CE81256
   :END:

[[file:../../openstack/readme.org::#h-9BEEAB97][Ensure the =/data= volume availability upon machine restart]].

** Sharing /data directory via NFS
   :PROPERTIES:
   :CUSTOM_ID: h-358A22F4
   :END:

Because volume multi-attach is not yet available via OpenStack, we will want to share the =/data= directory via NFS to client VMs over the ~10.0~ network by adding and an entry to the =/etc/exports= file. For example, here we are sharing the =/data= directory to the VM at ~10.0.0.18~.

#+BEGIN_SRC sh :exports none :tangle ../../../vms/idd-archiver/nfs-install.sh :shebang "#!/bin/bash"
  if [ "$EUID" -ne 0 ]
    then echo "Please run as root"
    exit
  fi
#+END_SRC

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/nfs-install.sh
  echo /data		10.0.0.18(rw,sync,no_subtree_check) | tee \
      --append /etc/exports > /dev/null
  echo /data		10.0.0.15(rw,sync,no_subtree_check) | tee \
      --append /etc/exports > /dev/null
  echo /data		10.0.0.11(rw,sync,no_subtree_check) | tee \
      --append /etc/exports > /dev/null
#+END_SRC

Now start NFS:

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/nfs-install.sh
  exportfs -a
  service nfs-kernel-server start
#+END_SRC

Finally, ensure NFS will be available when the VM starts:

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/nfs-install.sh
  update-rc.d nfs-kernel-server defaults
#+END_SRC

*** Open NFS Related Ports
     :PROPERTIES:
     :CUSTOM_ID: h-1AFDC551
     :END:

Via OpenStack also open NFS related ports: ~111~, ~1110~, ~2049~, ~4045~. If it does not exist already, create the ~local-nfs~ security group with the ~secgroup.sh~ convenience script and additional ~openstack~ commands.

#+BEGIN_SRC shell :tangle no
    # Will create a "local-nfs" security group.
  secgroup.sh  -p 111 -n local-nfs --remote-ip 10.0.0.0/24
  openstack security group rule create local-nfs --protocol tcp --dst-port 1110:1110 --remote-ip 10.0.0.0/24
  openstack security group rule create local-nfs --protocol tcp --dst-port 2049:2049 --remote-ip 10.0.0.0/24
  openstack security group rule create local-nfs --protocol tcp --dst-port 4045:4045 --remote-ip 10.0.0.0/24
#+END_SRC

Finally, attach the ~local-nfs~ security group to the newly created VM. The VM ID can be obtained with ~openstack server list~.

#+BEGIN_SRC shell :tangle no
  openstack server add security group <VM name or ID> local-nfs
#+END_SRC

** THREDDS Data Manager (TDM)
   :PROPERTIES:
   :CUSTOM_ID: h-DB469C8D
   :END:

While not related to IDD archival, the [[https://www.unidata.ucar.edu/software/thredds/current/tds/reference/collections/TDM.html][TDM]] is an application that works in conjunction with the TDS. It creates indexes for GRIB data as a background process, and notifies the TDS running on the ~tds.scigw~ VM via port ~8443~ when data have been updated or changed. Because the TDM needs to *write* data, and NFS tuning concerns, in the present configuration, we have the TDM running on the ~idd-archiver~ VM.

*** TDM Logging Directory
    :PROPERTIES:
    :CUSTOM_ID: h-865C1FF8
    :END:

Create a logging directory for the TDM:

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  mkdir -p /logs/tdm
#+END_SRC

**** Running the TDM Out the TDM Log Directory
     :PROPERTIES:
     :CUSTOM_ID: h-94768FE5
     :END:

[[https://github.com/Unidata/tdm-docker#capturing-tdm-log-files-outside-the-container][TDM logging will not be configurable until TDS 5.0]]. Until then we are running the TDM out of the =/logs/tdm= directory:

#+BEGIN_SRC shell :tangle ../../../vms/idd-archiver/idd-archiver-install.sh
  curl -SL  \
       https://artifacts.unidata.ucar.edu/content/repositories/unidata-releases/edu/ucar/tdmFat/4.6.13/tdmFat-4.6.13.jar \
       -o /logs/tdm/tdm.jar
  curl -SL https://raw.githubusercontent.com/Unidata/tdm-docker/master/tdm.sh \
       -o /logs/tdm/tdm.sh
  chmod +x  /logs/tdm/tdm.sh
#+END_SRC

*** Configuring the TDM to work with the TDS
    :PROPERTIES:
    :CUSTOM_ID: h-2C5BF1CA
    :END:

In the ~docker-compose.yml~ shown below, there is a reference to a ~compose.env~ file that contains TDM related environment variables.

#+INCLUDE: "../../../vms/idd-archiver/compose.env" src shell

Let's consider each environment variable (i.e., configuration option), in turn.

**** ~TDS_CONTENT_ROOT_PATH~
     :PROPERTIES:
     :CUSTOM_ID: h-DB951BAE
     :END:

This environment variable relates to the TDS content root *inside* the container and probably does not need to be changed.

**** ~TDM_PW~
     :PROPERTIES:
     :CUSTOM_ID: h-DB951BAE
     :END:

Supply the TDM password. For example,

#+BEGIN_EXAMPLE
  TDM_PW=CHANGEME!
#+END_EXAMPLE

Note that this password should correspond to the ~sha-512~ digested password of the ~tdm~ user in =~/science-gateway/vm/thredds/files/tomcat-users.xml= file on the *tds.scigw* VM. You can create a password/SHA pair with the following command:

  #+BEGIN_SRC shell
    docker run tomcat  /usr/local/tomcat/bin/digest.sh -a "sha-512" CHANGEME!
  #+END_SRC

Ensure you are using the correct hashing algorithm in the ~server.xml~ on the TDS server running on the tds.scigw VM. For example,

#+BEGIN_SRC xml
  <Realm className="org.apache.catalina.realm.UserDatabaseRealm"
         resourceName="UserDatabase">
    <CredentialHandler className="org.apache.catalina.realm.MessageDigestCredentialHandler" algorithm="sha-512" />
  </Realm>
#+END_SRC

**** ~TDS_HOST~
     :PROPERTIES:
     :CUSTOM_ID: h-02D6C753
     :END:

Supply the hostname of the TDS that the TDM will notify:

#+BEGIN_EXAMPLE
  TDS_HOST=https://tds.scigw.unidata.ucar.edu/
#+END_EXAMPLE

**** ~TDM_XMX_SIZE~, ~TDM_XMS_SIZE~
     :PROPERTIES:
     :CUSTOM_ID: h-F5791708
     :END:

Define the maximum and minimum size of the Java heap under which the TDM can operate:

#+BEGIN_EXAMPLE
  TDM_XMX_SIZE=6G

  TDM_XMS_SIZE=1G
#+END_EXAMPLE

** docker-compose.yml
   :PROPERTIES:
   :CUSTOM_ID: h-498535EC
   :END:

Based on the directory set we have defined, the =docker-compose.yml= file will look like this:

#+INCLUDE: "../../../vms/idd-archiver/docker-compose.yml" src yaml

*** LDM Environment Variable Parameterization
    :PROPERTIES:
    :CUSTOM_ID: h-A6C77825
    :END:

You can provide additional LDM parameterization via the =compose.env= file referenced in the =docker-compose.yml= file.

#+INCLUDE: "../../../vms/idd-archiver/compose.env" src shell


** Start the IDD Archiver Node
   :PROPERTIES:
   :CUSTOM_ID: h-4167D52C
   :END:

To start the IDD archiver node:

#+BEGIN_SRC shell :tangle no
  cd ~/science-gateway/vms/idd-archiver/
  docker-compose up -d
#+END_SRC
